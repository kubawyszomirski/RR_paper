{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7702da",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from environment_fx_no_env import calculate_import_export, test1, test2, test3, evaluate1, evaluate2, basepolicy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f3f975",
   "metadata": {
    "code_folding": [
     22
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# import and modify data\n",
    "\n",
    "# Assuming the file is a CSV and specifying the correct path and filename\n",
    "file_path = r\"file_path\"\n",
    "\n",
    "# Use pandas to read the CSV file\n",
    "AC_OUTPUT = pd.read_csv(file_path + \"/AC_OUTPUT_JA\")\n",
    "elec_df = pd.read_csv(file_path + \"/hourly_consumption2.csv\")\n",
    "import_price = pd.read_csv(file_path + \"/electricity_tariff.csv\")\n",
    "\n",
    "#elec_df = elec_df * 1000\n",
    "elec_df = elec_df.drop('HourOfYear', axis=1)\n",
    "\n",
    "elec_df['hour_of_day'] = np.arange(8760) % 24\n",
    "elec_df['day_of_week'] = np.arange(8760) // 24 % 7  # 0 is Monday, 6 is Sunday\n",
    "\n",
    "# Define rates\n",
    "peak_rate = 1.45\n",
    "normal_rate = 1\n",
    "off_peak_rate = 0.85\n",
    "\n",
    "# Function to determine rate based on hour and day\n",
    "def determine_rate(hour, day):\n",
    "    if day < 5:  # Monday to Friday\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return peak_rate\n",
    "        elif 6 <= hour < 10:  # 7am to 9am and 10am to 3pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    else:  # Weekend\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    \n",
    "# Apply the function to each row to determine the rate\n",
    "elec_df['rate'] = elec_df.apply(lambda row: determine_rate(row['hour_of_day'], row['day_of_week']), axis=1)\n",
    "\n",
    "import_price_df = import_price.drop(columns=['x'])\n",
    "import_price_df = import_price_df[:-26]\n",
    "\n",
    "train_cols = random.sample(list(import_price_df.columns), 7000)\n",
    "import_price_train = import_price_df[train_cols]\n",
    "test_cols = [col for col in import_price_df.columns if col not in train_cols]\n",
    "import_price_test = import_price_df[test_cols]\n",
    "\n",
    "Eff = pd.read_csv(file_path + \"/Efficency_impr\")\n",
    "Eff = (Eff)/100 + 1\n",
    "\n",
    "CAPEX = pd.read_csv(file_path + \"/CAPEX_JA.csv\")\n",
    "CAPEX_JA = (CAPEX[:26]) * 1.3\n",
    "\n",
    "\n",
    "train_cols_CAPEX = random.sample(list(CAPEX_JA.columns), 7000)\n",
    "test_cols_CAPEX = [col for col in CAPEX_JA.columns if col not in train_cols_CAPEX]\n",
    "\n",
    "CAPEX_JA_train = CAPEX_JA[train_cols_CAPEX]\n",
    "CAPEX_JA_test = CAPEX_JA[test_cols_CAPEX]\n",
    "\n",
    "train_cols_Eff = random.sample(list(Eff.columns), 7000)\n",
    "test_cols_Eff = [col for col in Eff.columns if col not in train_cols_Eff]\n",
    "\n",
    "Eff_train = Eff[train_cols_Eff]\n",
    "Eff_test = Eff[test_cols_Eff]\n",
    "\n",
    "AC_OUTPUT_arr = (np.array(AC_OUTPUT.T)).flatten()\n",
    "\n",
    "Eff_train_arr = np.array(Eff_train.T)\n",
    "Eff_test_arr = np.array(Eff_test.T)\n",
    "\n",
    "CAPEX_JA_train_arr = np.array(CAPEX_JA_train.T)\n",
    "CAPEX_JA_test_arr = np.array(CAPEX_JA_test.T)\n",
    "\n",
    "elec_consum_arr = np.array(elec_df[\"Consumption\"])\n",
    "import_price_rate = np.array(elec_df[\"rate\"])\n",
    "\n",
    "import_price_train_arr = np.array(import_price_train.T)\n",
    "import_price_test_arr = np.array(import_price_train.T)\n",
    "\n",
    "grid_factor = pd.read_csv(file_path + \"/grid_factor.csv\")\n",
    "grid_factor =  grid_factor.T\n",
    "\n",
    "train_cols_grid = random.sample(list(grid_factor.columns), 7000)\n",
    "grid_factor_train = grid_factor[train_cols_grid]\n",
    "test_cols_grid = [col for col in grid_factor.columns if col not in train_cols]\n",
    "grid_factor_test = grid_factor[test_cols_grid]\n",
    "\n",
    "grid_factor_train_arr = np.array(grid_factor_train.T)\n",
    "grid_factor_test_arr = np.array(grid_factor_test.T)\n",
    "\n",
    "pv_co2 = pd.read_csv(file_path + \"/pv_emissions.csv\")\n",
    "pv_co2_arr = np.array(pv_co2)\n",
    "pv_co2_arr = np.insert(pv_co2_arr, 0, 1.620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01482e1",
   "metadata": {
    "code_folding": [
     0,
     1,
     50,
     54,
     74,
     153,
     165,
     179,
     187,
     208,
     253,
     282,
     305
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class TrainEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX, \n",
    "                 GRID_FACTOR, pv_co2_arr):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 12\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 0.82 # Trina: 1.19, JA: 0.82, Maxeon: 0.67\n",
    "        self.deg_std = 0.555 \n",
    "        self.phi = 30 # Trina: 15, JA: 30, Maxeon: 50\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415  # Trina: 265, JA: 415, Maxeon: 435\n",
    "        self.cost_per_Wp_df_at_zero = 0.69 # Trina: 0.36, JA: 0.69, Maxeon: 1.58\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        \n",
    "        self.loan_interest_rate = 1.10\n",
    "        self.normal_interest_rate = 1.02\n",
    "        \n",
    "        self.low_budget = 0 # Low budget: 0, High Budget: 750\n",
    "        self.high_budget = 750 # Low budget: 750, High Budget: 2000\n",
    "                        \n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 8,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "        # Emission\n",
    "        self.grid_factor_df = GRID_FACTOR #****\n",
    "        self.grid_factor_at_zero = 0.553 \n",
    "        self.pv_emission = pv_co2_arr * self.power_at_zero\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "        excess_energy = (AC_OUTPUT_tot - self.elec_df - self.max_export).clip(min=0)\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "        AC_for_env = AC_OUTPUT_tot - excess_energy\n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, AC_for_env, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min()) / (self.import_price_df.max().max() - self.import_price_df.min().min())\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min() * 0.33) / (self.import_price_df.max().max() - self.import_price_df.min().min() * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "        \n",
    "        self.grid_factor_at_zero_norm = (self.grid_factor_at_zero + 0.05319002) / (0.55762151 + 0.05319002)\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(self.low_budget, self.high_budget)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0., self.grid_factor_at_zero_norm]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM Grid Factor\n",
    "        self.random_grid_factor = self.grid_factor_df[np.random.choice(self.grid_factor_df.shape[0])]   #***\n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def emission_balance(self, pv_production, grid_factor, panel_emission, action_step):\n",
    "        \n",
    "        curtailed = (pv_production.sum() * grid_factor)/1000\n",
    "        \n",
    "        number_installed = int(np.sum(action_step))\n",
    "        \n",
    "        panel_emission_tot = number_installed * panel_emission\n",
    "        \n",
    "        emission_balance = curtailed - panel_emission_tot\n",
    "        \n",
    "        return emission_balance \n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "    \n",
    "    def calculate_irr_and_npv(self, pv_cost, minimised_revenue, export_revenue, penalty):\n",
    "                \n",
    "        \"\"\"\n",
    "        Calculates total cash flow of the project needed for the internal rate of return\n",
    "        \"\"\" \n",
    "        self.expences = 0\n",
    "        self.annual_cash_flow = 0\n",
    "        initial_cost = 0\n",
    "        \n",
    "        self.expences = pv_cost\n",
    "        self.annual_cash_flow = self.expences + export_revenue + minimised_revenue + penalty\n",
    "        initial_cost_q, x = self.calculate_total_CAPEX(self.init_obs, self.panel_cost_and_inverter)\n",
    "        initial_cost = - initial_cost_q\n",
    "        \n",
    "        if self.episode_len == 24:\n",
    "            self.total_cash_flow.append(initial_cost + self.annual_cash_flow) \n",
    "        else:\n",
    "            self.total_cash_flow.append(self.annual_cash_flow) \n",
    "        \n",
    "        return self.total_cash_flow\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(self.low_budget, self.high_budget) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - self.low_budget * self.step_total_interest) / (self.high_budget * self.step_total_interest - self.low_budget * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * self.phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        self.env_balance = 0\n",
    "        irr_fin = 0\n",
    "        npv_fin = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "        self.grid_factor = self.random_grid_factor[abs(self.episode_len - 25)]\n",
    "        self.step_pv_emission = (self.pv_emission[abs(self.episode_len - 25)] * self.efficency_develop)\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(self.number_of_panels + 1)\n",
    "        \n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    "\n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "        \n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, AC_for_env, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP EMISSIONS\n",
    "        self.env_balance = self.emission_balance(AC_for_env, self.grid_factor, self.step_pv_emission, actions_step)\n",
    "        \n",
    "        self.env_balance_tot += self.env_balance\n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        # CALCULATE IRR, NPV AND CARBON INTENSITY\n",
    "        total_cash_flow = self.calculate_irr_and_npv(self.pv_costs, exported_revenue, minimised_revenue, current_penalty)\n",
    "        irr = npf.irr(total_cash_flow) * 100\n",
    "        npv = npf.npv(0.04 ,total_cash_flow)\n",
    "            \n",
    "        # RETURNS AND CALCULATE REWARD\n",
    "        if self.episode_len == 0:\n",
    "            irr_fin = irr\n",
    "            npv_fin = npv\n",
    "        \n",
    "        fin_mean = -1982\n",
    "        fin_stdev = 1142\n",
    "        \n",
    "        env_mean = -1797\n",
    "        st_dev = 1943\n",
    "        \n",
    "        reward = ((self.fin_balance - fin_mean) / fin_stdev) + ((self.env_balance - env_mean) / st_dev)\n",
    "        \n",
    "        # FAILURE\n",
    "         \n",
    "        survival = self.failure(actions_step)\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "\n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_grid_factor = self.random_grid_factor[abs(self.episode_len - 25)]\n",
    "\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "            \n",
    "            grid_factor_observation = (self.next_grid_factor - 0.553) / (0.553 - 0.00022499)\n",
    "\n",
    "            self.observation[self.number_of_panels + 7] = grid_factor_observation        \n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": irr_fin,\n",
    "               \"current_interest\": current_penalty,\n",
    "                \"net present value\": npv_fin,\n",
    "               \"enironmental balance\": self.env_balance_tot}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff95454",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TestEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX, \n",
    "                 GRID_FACTOR, pv_co2_arr):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 12\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 0.82 # Trina: 1.19, JA: 0.82, Maxeon: 0.67\n",
    "        self.deg_std = 0.555 \n",
    "        self.phi = 30 # Trina: 15, JA: 30, Maxeon: 50\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415  # Trina: 265, JA: 415, Maxeon: 435\n",
    "        self.cost_per_Wp_df_at_zero = 0.69 # Trina: 0.36, JA: 0.69, Maxeon: 1.58\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        \n",
    "        self.loan_interest_rate = 1.10\n",
    "        self.normal_interest_rate = 1.02\n",
    "        \n",
    "        self.low_budget = 0 # Low budget: 0, High Budget: 750\n",
    "        self.high_budget = 750 # Low budget: 750, High Budget: 2000\n",
    "                        \n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 8,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "        # Emission\n",
    "        self.grid_factor_df = GRID_FACTOR #****\n",
    "        self.grid_factor_at_zero = 0.553 \n",
    "        self.pv_emission = pv_co2_arr * self.power_at_zero\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "        excess_energy = (AC_OUTPUT_tot - self.elec_df - self.max_export).clip(min=0)\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "        AC_for_env = AC_OUTPUT_tot - excess_energy\n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, AC_for_env, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min()) / (self.import_price_df.max().max() - self.import_price_df.min().min())\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min() * 0.33) / (self.import_price_df.max().max() - self.import_price_df.min().min() * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "        \n",
    "        self.grid_factor_at_zero_norm = (self.grid_factor_at_zero + 0.05319002) / (0.55762151 + 0.05319002)\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(self.low_budget, self.high_budget)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0., self.grid_factor_at_zero_norm]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM Grid Factor\n",
    "        self.random_grid_factor = self.grid_factor_df[np.random.choice(self.grid_factor_df.shape[0])]   #***\n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def emission_balance(self, pv_production, grid_factor, panel_emission, action_step):\n",
    "        \n",
    "        curtailed = (pv_production.sum() * grid_factor)/1000\n",
    "        \n",
    "        number_installed = int(np.sum(action_step))\n",
    "        \n",
    "        panel_emission_tot = number_installed * panel_emission\n",
    "        \n",
    "        emission_balance = curtailed - panel_emission_tot\n",
    "        \n",
    "        return emission_balance \n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "    \n",
    "    def calculate_irr_and_npv(self, pv_cost, minimised_revenue, export_revenue, penalty):\n",
    "                \n",
    "        \"\"\"\n",
    "        Calculates total cash flow of the project needed for the internal rate of return\n",
    "        \"\"\" \n",
    "        self.expences = 0\n",
    "        self.annual_cash_flow = 0\n",
    "        initial_cost = 0\n",
    "        \n",
    "        self.expences = pv_cost\n",
    "        self.annual_cash_flow = self.expences + export_revenue + minimised_revenue + penalty\n",
    "        initial_cost_q, x = self.calculate_total_CAPEX(self.init_obs, self.panel_cost_and_inverter)\n",
    "        initial_cost = - initial_cost_q\n",
    "        \n",
    "        if self.episode_len == 24:\n",
    "            self.total_cash_flow.append(initial_cost + self.annual_cash_flow) \n",
    "        else:\n",
    "            self.total_cash_flow.append(self.annual_cash_flow) \n",
    "        \n",
    "        return self.total_cash_flow\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(self.low_budget, self.high_budget) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - self.low_budget * self.step_total_interest) / (self.high_budget * self.step_total_interest - self.low_budget * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * self.phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        self.env_balance = 0\n",
    "        irr_fin = 0\n",
    "        npv_fin = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "        self.grid_factor = self.random_grid_factor[abs(self.episode_len - 25)]\n",
    "        self.step_pv_emission = (self.pv_emission[abs(self.episode_len - 25)] * self.efficency_develop)\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(self.number_of_panels + 1)\n",
    "        \n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    "\n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "        \n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, AC_for_env, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP EMISSIONS\n",
    "        self.env_balance = self.emission_balance(AC_for_env, self.grid_factor, self.step_pv_emission, actions_step)\n",
    "        \n",
    "        self.env_balance_tot += self.env_balance\n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        # CALCULATE IRR, NPV AND CARBON INTENSITY\n",
    "        total_cash_flow = self.calculate_irr_and_npv(self.pv_costs, exported_revenue, minimised_revenue, current_penalty)\n",
    "        irr = npf.irr(total_cash_flow) * 100\n",
    "        npv = npf.npv(0.04 ,total_cash_flow)\n",
    "            \n",
    "        # RETURNS AND CALCULATE REWARD\n",
    "        if self.episode_len == 0:\n",
    "            irr_fin = irr\n",
    "            npv_fin = npv\n",
    "        \n",
    "        fin_mean = -1982\n",
    "        fin_stdev = 1142\n",
    "        \n",
    "        env_mean = -1797\n",
    "        st_dev = 1943\n",
    "        \n",
    "        reward = ((self.fin_balance - fin_mean) / fin_stdev) + ((self.env_balance - env_mean) / st_dev)\n",
    "        # FAILURE\n",
    "         \n",
    "        survival = self.failure(actions_step)\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "\n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "            self.next_grid_factor = self.random_grid_factor[abs(self.episode_len - 25)]\n",
    "\n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "            \n",
    "            grid_factor_observation = (self.next_grid_factor - 0.553) / (0.553 - 0.00022499)\n",
    "\n",
    "            self.observation[self.number_of_panels + 7] = grid_factor_observation        \n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": irr_fin,\n",
    "               \"current_interest\": current_penalty,\n",
    "                \"net present value\": npv_fin,\n",
    "               \"enironmental balance\": self.env_balance_tot}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c69028",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, \n",
    "                       Eff_train_arr, CAPEX_JA_train_arr, grid_factor_train_arr, pv_co2_arr)\n",
    "env_test = TestEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_test_arr, Eff_test_arr, CAPEX_JA_test_arr, \n",
    "                           grid_factor_test_arr, pv_co2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4903b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.        , 0.8519587 , 0.        , 0.        , 0.98594385,\n",
      "       0.8568305 , 0.        , 0.        , 0.8833164 , 0.        ,\n",
      "       0.        , 0.        , 0.976149  , 0.976987  , 0.8906104 ,\n",
      "       0.8831296 , 0.9516209 , 0.8663963 , 0.        , 0.        ,\n",
      "       0.        , 0.8794743 , 0.9527075 , 0.        , 0.06935652,\n",
      "       0.11512984, 0.00636943, 0.49593392, 0.        , 0.        ,\n",
      "       0.        , 0.99243385], dtype=float32), {}) \n",
      "\n",
      "STEP: 1\n",
      "ACT \n",
      " 1\n",
      "OBS \n",
      " [ 0.9832116   0.8432891   0.          0.          0.97749156  0.8416709\n",
      "  0.          0.          0.87606835  0.          0.          0.\n",
      "  0.9698822   0.96612513  0.8822917   0.88039523  0.9375154   0.8598404\n",
      "  0.          0.          0.          0.8658387   0.9462492   0.\n",
      "  0.05286848  0.06606951  0.0064711   0.49593392  0.06363465  0.01644833\n",
      "  0.44933334 -0.08507548]\n",
      "step financial balance (eur): 1172.9835921597867 current_interest 0\n",
      "\n",
      "\n",
      "STEP: 2\n",
      "ACT \n",
      " 17\n",
      "OBS \n",
      " [ 0.97573143  0.99814487  0.98770964  0.99006605  0.9766235   0.98897535\n",
      "  0.98041064  0.9935813   0.9883435   0.996373    0.99106294  0.982998\n",
      "  0.9638233   0.95513415  0.8687549   0.99130505  0.9242118   0.9884208\n",
      "  0.9897226   0.9998094   0.9853895   0.9989316   0.92527956  0.98951524\n",
      "  0.0366643   0.05647833  0.00678781  0.49593392  0.7776637   0.2503712\n",
      "  0.168      -0.20245788]\n",
      "step financial balance (eur): -5467.953197381487 current_interest -131.58662548828124\n",
      "\n",
      "\n",
      "STEP: 3\n",
      "ACT \n",
      " 23\n",
      "OBS \n",
      " [ 0.9874828   0.9979514   1.0008937   1.0000544   0.9871256   0.9855925\n",
      "  0.9850419   1.0048263   0.9817967   0.99557984  0.9972419   0.98641944\n",
      "  0.9914906   0.9852466   0.98294383  0.9919432   1.0042671   0.98445684\n",
      "  0.9841635   1.0016242   0.9849302   0.994643    0.9955919   0.99663454\n",
      "  0.02830018  0.02662346  0.00678918  0.49593392  0.5386901   0.43965238\n",
      "  0.24266666 -0.24369447]\n",
      "step financial balance (eur): -5142.667018789012 current_interest -2002.96949386404\n",
      "\n",
      "\n",
      "STEP: 4\n",
      "ACT \n",
      " 0\n",
      "OBS \n",
      " [ 0.9791484   0.9921645   0.9923341   0.9866104   0.98788583  0.9785502\n",
      "  0.9751913   0.9946112   0.9756415   0.98466706  0.9874206   0.9815045\n",
      "  0.9711827   0.9746071   0.9765819   0.98691624  0.9903539   0.9756904\n",
      "  0.9618215   0.9958436   0.98362666  0.9865967   0.9864203   1.0001919\n",
      "  0.0236472   0.03670876  0.015922    0.49593392  0.04364366  0.49375498\n",
      "  0.09866667 -0.31082374]\n",
      "step financial balance (eur): -2196.03779991606 current_interest -3517.219070851813\n",
      "\n",
      "\n",
      "STEP: 5\n",
      "ACT \n",
      " 19\n",
      "OBS \n",
      " [ 0.9940402   0.98632467  0.98727787  1.0021211   1.0032833   0.99877596\n",
      "  0.99529177  0.98926175  0.9863418   0.99051225  0.995897    0.9898768\n",
      "  0.99423313  1.0008719   1.0057784   0.9951772   0.9861302   0.99795794\n",
      "  0.99487484  0.99000317  1.0025312   1.0019654   0.996244    0.99234885\n",
      "  0.02640107  0.01352152  0.01627008  0.49593392  0.44715375  0.65628093\n",
      "  0.784      -0.36412027]\n",
      "step financial balance (eur): -6530.832036428103 current_interest -3950.0399082409945\n",
      "\n",
      "\n",
      "STEP: 6\n",
      "ACT \n",
      " 11\n",
      "OBS \n",
      " [ 0.994521    0.9958197   0.98920953  0.9974589   0.990245    0.9884975\n",
      "  0.98068136  0.9896337   0.99920654  0.99815863  0.98363173  0.9827702\n",
      "  1.000696    0.9925431   0.99601513  0.99544525  0.9907008   0.9902748\n",
      "  0.99831074  0.9843538   0.99912196  1.0008833   0.98571223  0.9902603\n",
      "  0.02326061  0.00709905  0.16418923  0.49593392  0.2834307   0.45011535\n",
      "  0.5506667  -0.43577057]\n",
      "step financial balance (eur): -6507.046118450039 current_interest -5250.247259491631\n",
      "\n",
      "\n",
      "STEP: 7\n",
      "ACT \n",
      " 1\n",
      "OBS \n",
      " [ 0.983856    0.98660654  0.9793435   0.9880122   0.9796399   0.9859672\n",
      "  1.0200411   0.9796985   0.9920339   0.98197234  0.96366     0.9803893\n",
      "  0.99607605  0.98834383  0.98676467  0.9899391   0.98050326  0.9837647\n",
      "  0.99189484  0.97502065  1.0025071   0.9850466   0.97381514  0.9757068\n",
      "  0.04568264  0.01394217  0.1647213   0.49593392  0.06900594  0.25522393\n",
      "  0.76       -0.46809828]\n",
      "step financial balance (eur): -2834.5200081797884 current_interest -3600.9228115893675\n",
      "\n",
      "\n",
      "STEP: 8\n",
      "ACT \n",
      " 4\n",
      "OBS \n",
      " [ 0.96923506  0.9759633   0.9806881   0.96902984  0.9782758   0.97345686\n",
      "  1.016887    0.97108877  0.9881738   0.9667853   1.0185332   0.9740255\n",
      "  0.98851657  0.9703612   0.98343885  0.9904648   0.9736434   0.9755084\n",
      "  0.9819589   1.0201331   0.9907902   0.9772538   1.0180334   1.0141398\n",
      "  0.05005993  0.0152781   0.16472188  0.49593392  0.128858    0.32036066\n",
      "  0.004      -0.5013267 ]\n",
      "step financial balance (eur): -1647.6881545257881 current_interest -2041.7915149320775\n",
      "\n",
      "\n",
      "STEP: 9\n",
      "ACT \n",
      " 5\n",
      "OBS \n",
      " [ 1.0157717   0.96284133  0.9702819   1.0209082   0.9649376   0.9713696\n",
      "  1.0150605   1.0223327   0.9723798   1.0195874   1.0116885   0.97351444\n",
      "  0.97530335  1.0167991   0.96388525  0.9777734   0.96714985  0.9632731\n",
      "  0.97803026  1.005643    0.9777192   0.96947473  1.0069547   1.011456\n",
      "  0.04892147  0.01493065  0.16750194  0.49593392  0.16475913  0.18583997\n",
      "  0.996      -0.52534837]\n",
      "step financial balance (eur): -2476.206688635938 current_interest -2562.88526270327\n",
      "\n",
      "\n",
      "STEP: 10\n",
      "ACT \n",
      " 15\n",
      "OBS \n",
      " [ 1.0169135   1.0151883   1.013173    1.0060848   1.0141708   1.0115106\n",
      "  1.016475    1.0162051   1.0178887   1.0132174   1.0039827   1.0203023\n",
      "  1.013012    1.0141003   1.0164031   1.0166918   1.0162989   1.0223576\n",
      "  1.0093321   1.0183527   1.0250834   1.0126505   0.99877316  1.0037941\n",
      "  0.06853969  0.02091805  0.16750212  0.49593392  0.335344    0.13341218\n",
      "  0.34666666 -0.55623937]\n",
      "step financial balance (eur): -3064.8238770943067 current_interest -1486.7196858763286\n",
      "\n",
      "\n",
      "STEP: 11\n",
      "ACT \n",
      " 23\n",
      "OBS \n",
      " [ 1.0144769   1.0146073   1.0177201   1.0080733   1.0123421   1.0189197\n",
      "  1.0161      1.0158992   1.018489    1.0119929   1.0035237   1.0104063\n",
      "  1.0119889   1.0131519   1.0121365   1.0163317   1.019676    1.0248712\n",
      "  1.0187473   1.0157777   1.0145475   1.0284016   1.0208184   1.0190785\n",
      "  0.08550859  0.0260969   0.16750312  0.49593392  0.5213358   0.2997665\n",
      "  0.156      -0.58733624]\n",
      "step financial balance (eur): -4291.80170842919 current_interest -1067.29743712312\n",
      "\n",
      "\n",
      "STEP: 12\n",
      "ACT \n",
      " 4\n",
      "OBS \n",
      " [ 1.0127132   1.0117531   1.0112768   1.0270723   1.0009935   1.0101902\n",
      "  0.9981627   1.0066748   1.0133895   1.0011921   1.0122031   1.0195135\n",
      "  1.0188928   1.0019666   1.0010414   1.0118419   1.016626    1.0156503\n",
      "  1.0197717   1.0130955   1.0113366   1.0101293   1.0063099   1.0077457\n",
      "  0.08731721  0.02664888  0.16750312  0.49593392  0.14225385  0.3696514\n",
      "  0.87733334 -0.6253304 ]\n",
      "step financial balance (eur): -1672.0846284908648 current_interest -2398.1320216691797\n",
      "\n",
      "\n",
      "STEP: 13\n",
      "ACT \n",
      " 21\n",
      "OBS \n",
      " [ 1.0152954   1.009249    1.0174737   1.0249853   1.0134726   1.0213577\n",
      "  1.0184635   1.0098397   1.0165982   1.015371    1.0179498   1.0117851\n",
      "  1.0128878   1.007868    1.0101542   1.021664    1.015667    1.012342\n",
      "  1.0098535   1.0221554   1.015967    1.0119247   1.0147103   1.0155829\n",
      "  0.10218862  0.03118758  0.16751613  0.49593392  0.46135437  0.44948992\n",
      "  0.5373333  -0.6442806 ]\n",
      "step financial balance (eur): -5386.229676248673 current_interest -2957.211150379205\n",
      "\n",
      "\n",
      "STEP: 14\n",
      "ACT \n",
      " 9\n",
      "OBS \n",
      " [ 1.0110067   1.0130187   1.0121704   1.025622    1.0131866   1.0158858\n",
      "  1.0067692   1.0135381   1.009734    1.0037388   1.0093266   1.0185513\n",
      "  1.0123563   1.0105072   1.011461    1.0151671   1.0043162   1.0289173\n",
      "  1.0160174   1.0119518   1.0069236   1.0183337   0.99926883  1.0079004\n",
      "  0.15088493  0.04604951  0.16751614  0.49593392  0.22568594  0.43608943\n",
      "  0.19066666 -0.6120878 ]\n",
      "step financial balance (eur): -3524.9810675938284 current_interest -3595.9193802050722\n",
      "\n",
      "\n",
      "STEP: 15\n",
      "ACT \n",
      " 21\n",
      "OBS \n",
      " [ 1.008276    1.0116681   1.0119154   1.0219437   1.0184944   1.0192248\n",
      "  1.0170982   1.0174452   1.0188613   1.0180875   1.0182745   1.0088668\n",
      "  1.0262676   1.0143995   1.0076802   1.0111476   1.0223497   1.0149198\n",
      "  1.0043926   1.0184462   1.0130982   1.0043385   1.0113819   1.0084935\n",
      "  0.12604646  0.0384689   0.16772823  0.49593392  0.43653733  0.3861934\n",
      "  0.328      -0.6221002 ]\n",
      "step financial balance (eur): -5002.923499288373 current_interest -3488.7154852346575\n",
      "\n",
      "\n",
      "STEP: 16\n",
      "ACT \n",
      " 10\n",
      "OBS \n",
      " [ 1.0126209   1.0212197   1.0214752   1.0119838   1.0136591   1.0076334\n",
      "  1.0140059   1.0008675   1.007018    1.0095221   1.0149232   1.0230038\n",
      "  1.0230473   1.0141482   1.0096939   1.0152348   1.0082077   0.9970679\n",
      "  1.0098816   1.0077509   1.0071796   1.0129746   1.0159615   1.0185798\n",
      "  0.12365569  0.03773925  0.16772823  0.49593392  0.24616385  0.4329629\n",
      "  0.496      -0.60424256]\n",
      "step financial balance (eur): -2972.512536438783 current_interest -3089.5470563582753\n",
      "\n",
      "\n",
      "STEP: 17\n",
      "ACT \n",
      " 16\n",
      "OBS \n",
      " [ 1.0131154   1.0082134   1.0101794   1.0182852   1.0163699   1.023304\n",
      "  1.011733    1.0177966   1.0241207   1.0084716   1.0082438   1.0177264\n",
      "  1.0286688   1.0160693   1.0164498   1.0038809   1.013008    1.0082046\n",
      "  1.0206989   1.0179046   1.01674     1.0183333   1.0128534   1.0072805\n",
      "  0.14967354  0.0456798   0.16803889  0.49593392  0.3509119   0.3970576\n",
      "  0.396      -0.6248773 ]\n",
      "step financial balance (eur): -4412.918825515734 current_interest -3463.7032206579797\n",
      "\n",
      "\n",
      "STEP: 18\n",
      "ACT \n",
      " 0\n",
      "OBS \n",
      " [ 1.002015    0.9966239   1.0065402   1.0146263   1.0043677   1.0153384\n",
      "  0.9974718   1.007302    1.0146458   1.0000653   1.0007348   1.0144755\n",
      "  1.0218025   1.0131769   1.0052115   0.98751146  0.9923486   0.9951537\n",
      "  1.0103575   1.0077575   0.99518067  1.0134416   1.0012091   0.9990357\n",
      "  0.17126922  0.05227072  0.1706834   0.49593392  0.05758689  0.367357\n",
      "  0.28       -0.66126156]\n",
      "step financial balance (eur): -949.9114316319356 current_interest -3176.4607854418764\n",
      "\n",
      "\n",
      "STEP: 19\n",
      "ACT \n",
      " 16\n",
      "OBS \n",
      " [ 1.0253708   1.0126836   1.0164026   1.0013262   1.0204494   1.008719\n",
      "  1.0192249   1.0092639   1.0016439   1.0125288   1.003059    1.0133127\n",
      "  1.018248    0.9998126   1.0155      1.0181538   1.0174357   1.0186236\n",
      "  0.99870634  1.0173895   1.0146724   0.9995797   1.0109535   1.018142\n",
      "  0.17812516  0.05436312  0.18145812  0.49593392  0.35235006  0.3137327\n",
      "  0.028      -0.69508195]\n",
      "step financial balance (eur): -3409.3199323870867 current_interest -2938.855918300589\n",
      "\n",
      "\n",
      "STEP: 20\n",
      "ACT \n",
      " 2\n",
      "OBS \n",
      " [ 1.0127053   1.0030693   1.0021994   0.99257636  1.011651    1.0014821\n",
      "  1.0125254   1.0022322   1.0037714   1.0009973   0.99664754  1.0132649\n",
      "  1.0046626   0.99702686  1.0025297   1.0140694   1.0039139   0.9998846\n",
      "  1.0281415   1.0106046   1.001162    1.0177367   1.0011902   1.0048604\n",
      "  0.17159969  0.05237158  0.18147114  0.49593392  0.10874996  0.268555\n",
      "  0.66533333 -0.7360489 ]\n",
      "step financial balance (eur): -502.2690554303008 current_interest -2509.861597995321\n",
      "\n",
      "\n",
      "STEP: 21\n",
      "ACT \n",
      " 14\n",
      "OBS \n",
      " [ 1.0105575   1.0301174   1.0278237   1.0169495   1.002218    1.0219448\n",
      "  1.0132476   1.0197048   1.0219501   1.0196359   1.0068697   1.0072575\n",
      "  1.0029348   1.015954    1.0209029   1.0186192   1.0199814   1.0168496\n",
      "  1.0192724   1.0024644   1.0192884   1.0040705   1.019819    0.9953796\n",
      "  0.1937094   0.05911938  0.20788345  0.49593392  0.3352075   0.23980321\n",
      "  0.99866664 -0.74252725]\n",
      "step financial balance (eur): -2446.4847479427626 current_interest -2148.4397901777256\n",
      "\n",
      "\n",
      "STEP: 22\n",
      "ACT \n",
      " 7\n",
      "OBS \n",
      " [ 1.0037726   1.015722    1.0120541   1.0131217   1.0252105   1.0134968\n",
      "  1.0106927   1.0129013   1.0120145   1.0084039   1.0287654   1.0252781\n",
      "  1.0226313   1.0103378   1.0050327   1.0064269   1.0156502   0.9973177\n",
      "  1.0126829   1.0282477   1.0080281   1.0101818   1.0171154   1.0277612\n",
      "  0.1784396   0.05445909  0.21059647  0.49593392  0.211627    0.32509828\n",
      "  0.6906667  -0.7763529 ]\n",
      "step financial balance (eur): -743.4994138720522 current_interest -1918.4257019644547\n",
      "\n",
      "\n",
      "STEP: 23\n",
      "ACT \n",
      " 16\n",
      "OBS \n",
      " [ 1.0305037   1.0109311   1.0309794   1.0150841   1.019038    1.0293503\n",
      "  1.0267354   1.0189879   1.0247283   1.0270164   1.0298495   1.0233939\n",
      "  1.0153669   1.0233628   1.0323236   1.0303574   1.0193759   1.0221651\n",
      "  1.0222642   1.0218531   1.0258515   1.0234003   1.0076953   1.0109216\n",
      "  0.20255104  0.06181781  0.210644    0.49593392  0.36585212  0.299423\n",
      "  0.468      -0.8008199 ]\n",
      "step financial balance (eur): -3103.4035762875437 current_interest -2600.7862633038103\n",
      "\n",
      "\n",
      "STEP: 24\n",
      "ACT \n",
      " 24\n",
      "OBS \n",
      " [ 1.0206903   1.0288168   1.0261308   1.0276986   1.0241181   1.0226191\n",
      "  1.0197536   1.0114964   1.027775    1.0179212   1.0164821   1.019508\n",
      "  1.0305966   1.0223353   1.0106641   1.0249029   1.0252688   1.0150939\n",
      "  1.0191438   1.0197295   1.0313542   1.0248536   1.0172766   1.0119952\n",
      "  0.18843514  0.05750969  0.210644    0.49593392  0.48501164  0.3410872\n",
      "  0.232      -0.7806595 ]\n",
      "step financial balance (eur): -3824.169443521736 current_interest -2395.384058150291\n",
      "\n",
      "\n",
      "STEP: 25\n",
      "ACT \n",
      " 0\n",
      "OBS \n",
      " [ 1.0215718   1.0165343   1.0203594   1.0249493   1.0042357   1.0114154\n",
      "  1.0088372   0.9980226   1.0252938   1.0165744   1.0016785   1.010243\n",
      "  1.0135049   1.0133449   0.9918382   1.0115335   1.018135    1.0006641\n",
      "  1.0061327   1.0086836   1.0261031   1.015653    0.99499327  1.0058677\n",
      "  0.18843514  0.05750969  0.210644    0.49593392  0.06614923  0.2716544\n",
      "  0.8706667  -0.7806595 ]\n",
      "step financial balance (eur): -160.43021929892438 current_interest -2728.6975977849106\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test3(1, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1279284b",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test1(1000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e45550",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def make_env(rank: int, seed: int = 0) -> Callable:\n",
    "    def _init() -> gym.Env:\n",
    "        random.seed(seed + rank)\n",
    "        np.random.seed(seed + rank) \n",
    "        env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, Eff_train_arr, CAPEX_JA_train_arr)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "# Number of environments to run in parallel\n",
    "num_cpu = 16\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4053c411",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def logarithmic_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a logarithmically decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Avoid taking log of zero by setting a lower limit close to zero\n",
    "        epsilon = 0.0001\n",
    "        progress = max(epsilon, 1 - progress_remaining)\n",
    "        # Calculate the decay factor using a logarithmic scale\n",
    "        return final_value + (initial_value - final_value) * math.log(1/progress)\n",
    "    return func\n",
    "\n",
    "\n",
    "learning_rate = logarithmic_schedule(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36af5784",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "log_path = \"./logs/\"\n",
    "eval_callback = EvalCallback(env_test, best_model_save_path = \"C:/Users/kubaw/Desktop/DELFT/THESIS/CODE/TEST_MODELS/ja24_low/\",\n",
    "                             log_path = log_path, n_eval_episodes = 750, eval_freq=10000,\n",
    "                             deterministic=True, render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41010d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[1024, 1024], vf=[1024, 1024]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3408195a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a linearly decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Calculate the decrease based on the remaining progress\n",
    "        return final_value + (initial_value - final_value) * progress_remaining\n",
    "    return func\n",
    "\n",
    "# Define the learning rate using the linear schedule\n",
    "learning_rate = linear_schedule(0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678979b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\\PPO_387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x00000167E1791B70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000167E17933D0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 3921  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 8     |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1189        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016887046 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.2        |\n",
      "|    explained_variance   | -0.000337   |\n",
      "|    learning_rate        | 0.000299    |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 71          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 964         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017102802 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.000298    |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018324794 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.000297    |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_3616\\3881792178.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=160000, episode_reward=27.08 +/- 13.59\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 27.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017697046 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.99       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.000296    |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 67.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 5      |\n",
      "|    time_elapsed    | 242    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 662         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016355462 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.000295    |\n",
      "|    loss                 | 34.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 64          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017402146 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 37.3        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 66.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 669        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 391        |\n",
      "|    total_timesteps      | 262144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01638835 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.65      |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.000293   |\n",
      "|    loss                 | 28.2       |\n",
      "|    n_updates            | 168        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    value_loss           | 63.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 672         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017827239 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000292    |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 59.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=29.49 +/- 11.14\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 29.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014443753 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.000291    |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 10     |\n",
      "|    time_elapsed    | 502    |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013814912 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.00029     |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 604         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013437467 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00029     |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 264         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010376338 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.000289    |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013487228 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 51.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=480000, episode_reward=28.60 +/- 11.32\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 28.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015452577 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 619    |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 794    |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 849         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009749054 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.000286    |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 908          |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088140955 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.000285     |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 384          |\n",
      "|    policy_gradient_loss | -0.00635     |\n",
      "|    value_loss           | 48.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 964         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006121167 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.000284    |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 408         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 1013         |\n",
      "|    total_timesteps      | 622592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066305595 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.000283     |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 432          |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=31.79 +/- 12.82\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057508815 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.615        |\n",
      "|    learning_rate        | 0.000282     |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 456          |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    value_loss           | 47.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 1076   |\n",
      "|    total_timesteps | 655360 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1119        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006315012 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.000281    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 47.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 1163         |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077824574 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.00028      |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 504          |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    value_loss           | 50.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 1210         |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057234345 |\n",
      "|    clip_fraction        | 0.0829       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.000279     |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 528          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1256        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008123383 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.000278    |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 552         |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    value_loss           | 45.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=800000, episode_reward=34.49 +/- 13.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054384964 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.000277     |\n",
      "|    loss                 | 26.1         |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 47.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 619    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 1322   |\n",
      "|    total_timesteps | 819200 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 1367         |\n",
      "|    total_timesteps      | 851968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050972053 |\n",
      "|    clip_fraction        | 0.0844       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.621        |\n",
      "|    learning_rate        | 0.000276     |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    value_loss           | 46           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1410         |\n",
      "|    total_timesteps      | 884736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060210326 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.000275     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 624          |\n",
      "|    policy_gradient_loss | -0.00442     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1453         |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068431823 |\n",
      "|    clip_fraction        | 0.0617       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.646        |\n",
      "|    learning_rate        | 0.000274     |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 648          |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1495         |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061870776 |\n",
      "|    clip_fraction        | 0.0672       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.000273     |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 672          |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=35.26 +/- 12.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005680425 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.000272    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 696         |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 1557   |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1599        |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005958656 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1643        |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005210744 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 744         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 46.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 1685         |\n",
      "|    total_timesteps      | 1081344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068518445 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 0.00027      |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 768          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    value_loss           | 47.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 1727         |\n",
      "|    total_timesteps      | 1114112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065231416 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.000269     |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 792          |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    value_loss           | 46.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1120000, episode_reward=34.67 +/- 12.57\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1120000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066793975 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.000268     |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 816          |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 48           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 640     |\n",
      "|    iterations      | 35      |\n",
      "|    time_elapsed    | 1790    |\n",
      "|    total_timesteps | 1146880 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 1833         |\n",
      "|    total_timesteps      | 1179648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061630323 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.665        |\n",
      "|    learning_rate        | 0.000267     |\n",
      "|    loss                 | 22.6         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 45.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1875        |\n",
      "|    total_timesteps      | 1212416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009010244 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000266    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 864         |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 1918         |\n",
      "|    total_timesteps      | 1245184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075412076 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.000265     |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 888          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    value_loss           | 46.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 1959         |\n",
      "|    total_timesteps      | 1277952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062362296 |\n",
      "|    clip_fraction        | 0.0776       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.000264     |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 912          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 42.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=35.03 +/- 12.77\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838506 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 44.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 634     |\n",
      "|    iterations      | 40      |\n",
      "|    time_elapsed    | 2065    |\n",
      "|    total_timesteps | 1310720 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 630          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 2130         |\n",
      "|    total_timesteps      | 1343488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059271213 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.000262     |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 2192         |\n",
      "|    total_timesteps      | 1376256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058651688 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.000261     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 984          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 2254        |\n",
      "|    total_timesteps      | 1409024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006852123 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 1008        |\n",
      "|    policy_gradient_loss | -0.00296    |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=35.35 +/- 12.99\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007768878 |\n",
      "|    clip_fraction        | 0.0698      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.000259    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 1032        |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 613     |\n",
      "|    iterations      | 44      |\n",
      "|    time_elapsed    | 2348    |\n",
      "|    total_timesteps | 1441792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2411        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004517084 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.000258    |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 1056        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 45.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 2475         |\n",
      "|    total_timesteps      | 1507328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049355943 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.000257     |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 44.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 2537        |\n",
      "|    total_timesteps      | 1540096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006058248 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000256    |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 1104        |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 45.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 2599         |\n",
      "|    total_timesteps      | 1572864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055959094 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000255     |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 1128         |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=35.33 +/- 12.83\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018583365 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.000254    |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 1152        |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 595     |\n",
      "|    iterations      | 49      |\n",
      "|    time_elapsed    | 2697    |\n",
      "|    total_timesteps | 1605632 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 2764         |\n",
      "|    total_timesteps      | 1638400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061879624 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.67         |\n",
      "|    learning_rate        | 0.000253     |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 1176         |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    value_loss           | 45.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 2829        |\n",
      "|    total_timesteps      | 1671168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006634279 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2895        |\n",
      "|    total_timesteps      | 1703936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006809706 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 1224        |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 586          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 2961         |\n",
      "|    total_timesteps      | 1736704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051318146 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 0.000251     |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 1248         |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    value_loss           | 40.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=34.51 +/- 12.14\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1760000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052563483 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 1272         |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    value_loss           | 43           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 578     |\n",
      "|    iterations      | 54      |\n",
      "|    time_elapsed    | 3061    |\n",
      "|    total_timesteps | 1769472 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 576          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 3127         |\n",
      "|    total_timesteps      | 1802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055800956 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.685        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 1296         |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    value_loss           | 42.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 574          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 3191         |\n",
      "|    total_timesteps      | 1835008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049780365 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 42.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 573         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 3256        |\n",
      "|    total_timesteps      | 1867776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004672358 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 1344        |\n",
      "|    policy_gradient_loss | -0.00318    |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 572          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 3321         |\n",
      "|    total_timesteps      | 1900544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055733714 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 1368         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 39           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=35.15 +/- 13.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1920000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047842073 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.966       |\n",
      "|    explained_variance   | 0.681        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 1392         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 565     |\n",
      "|    iterations      | 59      |\n",
      "|    time_elapsed    | 3420    |\n",
      "|    total_timesteps | 1933312 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 564        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 3485       |\n",
      "|    total_timesteps      | 1966080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00657424 |\n",
      "|    clip_fraction        | 0.0746     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.957     |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.000244   |\n",
      "|    loss                 | 20.3       |\n",
      "|    n_updates            | 1416       |\n",
      "|    policy_gradient_loss | -0.000979  |\n",
      "|    value_loss           | 41.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 562         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 3550        |\n",
      "|    total_timesteps      | 1998848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007588461 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 3613         |\n",
      "|    total_timesteps      | 2031616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057988837 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.944       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 1464         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 3674         |\n",
      "|    total_timesteps      | 2064384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055770907 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.925       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 1488         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=34.49 +/- 12.72\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005003686 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 1512        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 556     |\n",
      "|    iterations      | 64      |\n",
      "|    time_elapsed    | 3768    |\n",
      "|    total_timesteps | 2097152 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 556         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 3829        |\n",
      "|    total_timesteps      | 2129920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003680475 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 1536        |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 3890         |\n",
      "|    total_timesteps      | 2162688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040362114 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.926       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 3950         |\n",
      "|    total_timesteps      | 2195456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045453343 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.953       |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 1584         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 4011        |\n",
      "|    total_timesteps      | 2228224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005406465 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000236    |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 1608        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 46.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=34.67 +/- 12.86\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004821037 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.000235    |\n",
      "|    loss                 | 19.3        |\n",
      "|    n_updates            | 1632        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 44.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 550     |\n",
      "|    iterations      | 69      |\n",
      "|    time_elapsed    | 4104    |\n",
      "|    total_timesteps | 2260992 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 550         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 4164        |\n",
      "|    total_timesteps      | 2293760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006450448 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.000234    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 1656        |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 550        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 4224       |\n",
      "|    total_timesteps      | 2326528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00727342 |\n",
      "|    clip_fraction        | 0.0532     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000233   |\n",
      "|    loss                 | 18.2       |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0022    |\n",
      "|    value_loss           | 41.1       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 550          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 4285         |\n",
      "|    total_timesteps      | 2359296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059439335 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.962       |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.000233     |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 1704         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 550          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 4347         |\n",
      "|    total_timesteps      | 2392064      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050271032 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.897       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.000232     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 1728         |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 41.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=35.35 +/- 12.71\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2400000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053279833 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.000231     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 1752         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 42.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 546     |\n",
      "|    iterations      | 74      |\n",
      "|    time_elapsed    | 4439    |\n",
      "|    total_timesteps | 2424832 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 4500         |\n",
      "|    total_timesteps      | 2457600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064306986 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.917       |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.00023      |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 1776         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 4562        |\n",
      "|    total_timesteps      | 2490368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004725259 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.901      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.000229    |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 4621         |\n",
      "|    total_timesteps      | 2523136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063987286 |\n",
      "|    clip_fraction        | 0.0606       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.895       |\n",
      "|    explained_variance   | 0.677        |\n",
      "|    learning_rate        | 0.000228     |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 1824         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 42.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 4680        |\n",
      "|    total_timesteps      | 2555904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005113746 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.000227    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 1848        |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=35.43 +/- 13.54\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006497221 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 1872        |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 38.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 542     |\n",
      "|    iterations      | 79      |\n",
      "|    time_elapsed    | 4771    |\n",
      "|    total_timesteps | 2588672 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 4831       |\n",
      "|    total_timesteps      | 2621440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00507019 |\n",
      "|    clip_fraction        | 0.0555     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.904     |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.000225   |\n",
      "|    loss                 | 19.4       |\n",
      "|    n_updates            | 1896       |\n",
      "|    policy_gradient_loss | -0.00253   |\n",
      "|    value_loss           | 42.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 4890       |\n",
      "|    total_timesteps      | 2654208    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00468238 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.906     |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.000224   |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.00269   |\n",
      "|    value_loss           | 42.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 4950         |\n",
      "|    total_timesteps      | 2686976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040484807 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.898       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.000223     |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 1944         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 5009        |\n",
      "|    total_timesteps      | 2719744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007855305 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.000222    |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 1968        |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=35.18 +/- 13.15\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2720000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048048487 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.951       |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.000221     |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 1992         |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 539     |\n",
      "|    iterations      | 84      |\n",
      "|    time_elapsed    | 5099    |\n",
      "|    total_timesteps | 2752512 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 5158        |\n",
      "|    total_timesteps      | 2785280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006319752 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00022     |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 2016        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 5217         |\n",
      "|    total_timesteps      | 2818048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067351796 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.912       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.000219     |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 5274        |\n",
      "|    total_timesteps      | 2850816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004491441 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000218    |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 2064        |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=35.59 +/- 12.91\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006303044 |\n",
      "|    clip_fraction        | 0.0533      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000217    |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 2088        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 537     |\n",
      "|    iterations      | 88      |\n",
      "|    time_elapsed    | 5364    |\n",
      "|    total_timesteps | 2883584 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 5423        |\n",
      "|    total_timesteps      | 2916352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006595499 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.000216    |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 2112        |\n",
      "|    policy_gradient_loss | -0.00229    |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 5481         |\n",
      "|    total_timesteps      | 2949120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065518944 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000215     |\n",
      "|    loss                 | 22.5         |\n",
      "|    n_updates            | 2136         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 5540         |\n",
      "|    total_timesteps      | 2981888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038122335 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.744       |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 43.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 5596         |\n",
      "|    total_timesteps      | 3014656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044019017 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.729       |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 2184         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=34.01 +/- 12.78\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003953959 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.000213    |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 2208        |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 536     |\n",
      "|    iterations      | 93      |\n",
      "|    time_elapsed    | 5683    |\n",
      "|    total_timesteps | 3047424 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 5739       |\n",
      "|    total_timesteps      | 3080192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00444441 |\n",
      "|    clip_fraction        | 0.0465     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.733     |\n",
      "|    explained_variance   | 0.693      |\n",
      "|    learning_rate        | 0.000212   |\n",
      "|    loss                 | 18.6       |\n",
      "|    n_updates            | 2232       |\n",
      "|    policy_gradient_loss | -0.00217   |\n",
      "|    value_loss           | 39.9       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 5796        |\n",
      "|    total_timesteps      | 3112960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004878043 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.000211    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 2256        |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 5854        |\n",
      "|    total_timesteps      | 3145728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005810477 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00021     |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00258    |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 5910         |\n",
      "|    total_timesteps      | 3178496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042867744 |\n",
      "|    clip_fraction        | 0.0541       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.000209     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 2304         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=34.60 +/- 12.25\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035629503 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 2328         |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    value_loss           | 38.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 535     |\n",
      "|    iterations      | 98      |\n",
      "|    time_elapsed    | 5998    |\n",
      "|    total_timesteps | 3211264 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 6054        |\n",
      "|    total_timesteps      | 3244032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006700354 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.000207    |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 2352        |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 6110        |\n",
      "|    total_timesteps      | 3276800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004625962 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.000206    |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 2376        |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 6166        |\n",
      "|    total_timesteps      | 3309568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005060712 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.000205    |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 6223         |\n",
      "|    total_timesteps      | 3342336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044405246 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.000204     |\n",
      "|    loss                 | 19.2         |\n",
      "|    n_updates            | 2424         |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=35.67 +/- 13.35\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3360000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039913463 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.000203     |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 2448         |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 40.6         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 534     |\n",
      "|    iterations      | 103     |\n",
      "|    time_elapsed    | 6311    |\n",
      "|    total_timesteps | 3375104 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 6368         |\n",
      "|    total_timesteps      | 3407872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038336336 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.000202     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 2472         |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 6424        |\n",
      "|    total_timesteps      | 3440640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004869872 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.000201    |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 2496        |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 6482         |\n",
      "|    total_timesteps      | 3473408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067307157 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.647       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0002       |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 6540         |\n",
      "|    total_timesteps      | 3506176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051612416 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.000199     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 2544         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=34.85 +/- 13.81\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038354218 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.000198     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 2568         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 40.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 533     |\n",
      "|    iterations      | 108     |\n",
      "|    time_elapsed    | 6628    |\n",
      "|    total_timesteps | 3538944 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 6686         |\n",
      "|    total_timesteps      | 3571712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038604266 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000197     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2592         |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 42.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 6744        |\n",
      "|    total_timesteps      | 3604480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004257274 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.000196    |\n",
      "|    loss                 | 20.2        |\n",
      "|    n_updates            | 2616        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 6801         |\n",
      "|    total_timesteps      | 3637248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066052703 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.000195     |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 42           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 6859       |\n",
      "|    total_timesteps      | 3670016    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00442305 |\n",
      "|    clip_fraction        | 0.0438     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.675      |\n",
      "|    learning_rate        | 0.000195   |\n",
      "|    loss                 | 18.5       |\n",
      "|    n_updates            | 2664       |\n",
      "|    policy_gradient_loss | -0.00211   |\n",
      "|    value_loss           | 40.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=35.50 +/- 13.85\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3680000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048039816 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.000194     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 2688         |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 533     |\n",
      "|    iterations      | 113     |\n",
      "|    time_elapsed    | 6946    |\n",
      "|    total_timesteps | 3702784 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 533         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 7001        |\n",
      "|    total_timesteps      | 3735552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595237 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000193    |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 2712        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 533          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 7058         |\n",
      "|    total_timesteps      | 3768320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035918856 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.000192     |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 2736         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 7112         |\n",
      "|    total_timesteps      | 3801088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040644268 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.686        |\n",
      "|    learning_rate        | 0.000191     |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 7161        |\n",
      "|    total_timesteps      | 3833856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004201303 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00019     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 2784        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=35.05 +/- 12.99\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005920025 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.000189    |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 2808        |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    value_loss           | 41          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 535     |\n",
      "|    iterations      | 118     |\n",
      "|    time_elapsed    | 7226    |\n",
      "|    total_timesteps | 3866624 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 7275         |\n",
      "|    total_timesteps      | 3899392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034047742 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.000188     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2832         |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 7327         |\n",
      "|    total_timesteps      | 3932160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040518213 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.000187     |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 2856         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 40.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 7370         |\n",
      "|    total_timesteps      | 3964928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037208232 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.000186     |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 40.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 7420         |\n",
      "|    total_timesteps      | 3997696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043428093 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.562       |\n",
      "|    explained_variance   | 0.695        |\n",
      "|    learning_rate        | 0.000185     |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 2904         |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, learning_rate \u001b[38;5;241m=\u001b[39m learning_rate, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m, n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m, policy_kwargs \u001b[38;5;241m=\u001b[39m policy_kwargs, gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m,  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/kubaw/Desktop/DELFT/THESIS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE/TEST_MODELS/LOGS/logs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    208\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:504\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    502\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:515\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    510\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[0;32m    512\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[1;32m--> 515\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_torch, data)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, learning_rate = learning_rate, batch_size = 1024, n_epochs = 24, policy_kwargs = policy_kwargs, gamma = 0.99,  verbose=1, tensorboard_log = \"C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\")\n",
    "TIMESTEPS = 10000000\n",
    "model.learn(total_timesteps = TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5c9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c69a9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_low.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d69968a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act: 10 \n",
      " Obs: [0.9920464  0.9994238  0.90680546 0.9793497  0.9131121  0.9879631\n",
      " 0.         0.9801772  0.85552186 0.85957336 0.991444   0.99109715\n",
      " 0.9910932  0.873466   0.994912   0.8703445  0.88702345 0.86262\n",
      " 0.87649864 0.8431656  0.9797953  0.987805   0.9002548  0.849964\n",
      " 0.08038935 0.08235899 0.00987493 0.7273503  0.27679935 0.08496854\n",
      " 0.21333334] \n",
      " Balance 466.240085314098\n",
      "Act: 4 \n",
      " Obs: [0.99284405 0.98493433 0.8983833  0.9729517  0.9070731  0.97648704\n",
      " 0.9972731  0.9627195  0.9874343  0.8479211  0.97174716 0.9770742\n",
      " 0.98819536 0.8540514  0.98781574 0.86700916 0.87814903 0.8426237\n",
      " 0.8688962  0.9991762  0.96836436 0.9796356  0.8922252  0.99580985\n",
      " 0.07348731 0.07827371 0.01147796 0.69803023 0.14883503 0.13147761\n",
      " 0.844     ] \n",
      " Balance 551.1074099740363\n",
      "Act: 3 \n",
      " Obs: [0.9849447  0.97168195 0.88615644 0.96360284 0.89566857 0.9636939\n",
      " 0.99356717 0.95989376 0.970958   0.9942695  0.9651027  0.9602139\n",
      " 0.9713039  0.98739725 0.9737528  0.866173   0.8744647  0.9944171\n",
      " 0.8544764  0.98116493 0.96236634 0.9687749  0.8901805  0.99473953\n",
      " 0.05341931 0.03730977 0.04574391 0.6975262  0.10197125 0.16193746\n",
      " 0.21733333] \n",
      " Balance 582.8615943650957\n",
      "Act: 0 \n",
      " Obs: [0.9809549  0.96234447 0.87086535 0.9599421  0.88224083 0.95073295\n",
      " 0.9867767  0.9404851  0.9627346  0.97828925 0.95191467 0.95610726\n",
      " 0.94963294 0.98627967 0.9690344  0.8569201  0.8645532  0.9884079\n",
      " 0.84244967 0.9662455  0.9522643  0.95862174 0.8757216  0.98735964\n",
      " 0.05316971 0.0517257  0.04575227 0.6612168  0.04364366 0.18263121\n",
      " 0.296     ] \n",
      " Balance 223.4050957576053\n",
      "Act: 2 \n",
      " Obs: [0.96182036 0.9563179  0.85863715 0.9485199  0.87056303 0.9488942\n",
      " 0.96861935 0.927226   0.9554391  0.97589403 0.95065963 0.94584495\n",
      " 0.933231   0.972076   0.9609537  0.9926035  0.8526679  0.9835813\n",
      " 0.9909536  0.9571711  0.9428246  0.9513429  0.866294   0.97542703\n",
      " 0.06163012 0.02557653 0.30078927 0.65270823 0.08633781 0.08042844\n",
      " 0.16      ] \n",
      " Balance -165.1523963760228\n",
      "Act: 5 \n",
      " Obs: [0.95889914 0.95003664 1.0322552  0.92698866 1.0384542  0.9399247\n",
      " 0.9564182  1.0408577  0.93868154 0.97488135 0.9365763  0.93044525\n",
      " 0.9209519  0.95641553 0.94984615 0.9847489  1.0407443  0.98656964\n",
      " 0.9876115  0.9467895  0.9293849  0.9334044  1.0412207  0.9648593\n",
      " 0.04967139 0.01515952 0.30078927 0.6305007  0.1608244  0.07909109\n",
      " 0.924     ] \n",
      " Balance -347.923864854253\n",
      "Act: 1 \n",
      " Obs: [0.94433105 0.9386983  1.0245011  0.91159856 1.0171973  0.9345484\n",
      " 0.94087887 1.0323733  0.92591894 0.9662679  0.9331646  0.9227377\n",
      " 1.0290654  0.93773097 0.94312257 0.96274793 1.032564   0.97365415\n",
      " 0.97122157 0.93352157 0.90861917 0.92280954 1.0370574  0.9492606\n",
      " 0.05452393 0.0166405  0.32363844 0.56746656 0.06875557 0.07076219\n",
      " 0.5226667 ] \n",
      " Balance 360.34630450307054\n",
      "Act: 3 \n",
      " Obs: [0.9341977  0.9197671  1.0000215  1.0489156  1.0000823  0.92515063\n",
      " 0.93803    1.0134736  0.92840904 0.9501421  0.92745304 1.0358223\n",
      " 1.0031229  0.9257981  0.933679   0.94989187 1.0159355  0.9618192\n",
      " 0.9525415  0.92806876 1.0410211  0.9059273  1.0297701  0.94369173\n",
      " 0.06719078 0.02050637 0.35846838 0.5325156  0.1094835  0.10186418\n",
      " 0.9026667 ] \n",
      " Balance 77.32535578733291\n",
      "Act: 1 \n",
      " Obs: [0.9313299  0.9116691  0.99251425 1.0453085  0.9842661  0.9265605\n",
      " 0.9310558  1.0006756  0.9247395  0.9394804  0.91071093 1.030995\n",
      " 0.99652416 0.91144526 0.92682683 0.93924034 1.0071552  0.93957037\n",
      " 0.94262195 0.91666704 1.0228834  1.0337741  1.0138144  0.936942\n",
      " 0.05784584 0.01765433 0.35846838 0.46815774 0.07081048 0.0833941\n",
      " 0.948     ] \n",
      " Balance 326.8511035740812\n",
      "Act: 1 \n",
      " Obs: [0.9280648  0.9040743  0.98246306 1.0402855  0.980211   0.9110034\n",
      " 0.923745   0.9901983  0.9215951  0.9261194  1.0450807  1.0159867\n",
      " 0.9862168  0.90643024 0.9145726  0.9247846  1.0042862  0.9237751\n",
      " 0.9235151  0.9027918  1.011395   1.0244827  1.0003653  0.9277995\n",
      " 0.05341056 0.0163007  0.35846838 0.45807335 0.07176661 0.03017569\n",
      " 0.05333333] \n",
      " Balance 363.90261985751727\n",
      "Act: 0 \n",
      " Obs: [0.91918874 0.8942188  0.9608295  1.0231557  0.9652262  0.89187217\n",
      " 0.91123194 0.98607713 0.9160319  0.9117958  1.0365926  1.0036873\n",
      " 0.97172225 0.89494103 0.90538156 0.9306384  0.9905598  0.92052454\n",
      " 0.908801   0.9063611  1.0041125  1.0152072  0.9877755  0.91638595\n",
      " 0.05812461 0.01773941 0.35927314 0.42818886 0.05013285 0.01414277\n",
      " 0.48533332] \n",
      " Balance 940.6015045993839\n",
      "Act: 0 \n",
      " Obs: [0.9032155  0.8903964  0.9518861  1.0055987  0.95587635 0.8798041\n",
      " 0.9099257  0.97930205 0.89847016 0.9037408  1.0297076  1.0000904\n",
      " 0.9609857  0.88461393 0.9019166  0.9254296  0.973966   0.90110135\n",
      " 0.8924436  0.90081155 0.98156345 1.0095412  0.9679526  0.8998115\n",
      " 0.07271033 0.02219092 0.36395857 0.40701917 0.05113551 0.01866427\n",
      " 0.404     ] \n",
      " Balance 1081.2125311410389\n",
      "Act: 0 \n",
      " Obs: [0.894867   0.88546705 0.93768966 1.0064123  0.9379841  0.8766662\n",
      " 0.89994615 0.960425   0.8792838  0.8906217  1.0238352  0.99177116\n",
      " 0.95852524 0.8757069  0.89223737 0.92623407 0.9575421  0.88372755\n",
      " 0.8714135  0.891993   0.95966095 1.0020888  0.9580125  0.88531137\n",
      " 0.0781537  0.02385221 0.36420298 0.388344   0.05215822 0.02836568\n",
      " 0.21733333] \n",
      " Balance 1146.9221064503395\n",
      "Act: 0 \n",
      " Obs: [0.87535286 0.873597   0.9361316  1.0012665  0.93182194 0.8696274\n",
      " 0.89294016 0.9509239  0.8710764  0.87831295 1.0081687  0.9761928\n",
      " 0.9434977  0.8592608  0.8849984  0.92408866 0.95173126 0.8685459\n",
      " 0.85984683 0.88244855 0.944903   0.9912685  0.94717264 0.87579006\n",
      " 0.09254457 0.02824426 0.36420298 0.3669739  0.05320138 0.03932222\n",
      " 0.9026667 ] \n",
      " Balance 1083.075820982599\n",
      "Act: 2 \n",
      " Obs: [0.87247515 0.8566841  0.92785525 0.98865443 0.9246621  0.8429431\n",
      " 0.8816036  0.9330588  0.84687275 0.8665231  0.9931872  0.97582215\n",
      " 0.936392   1.0536146  0.87791055 0.9119171  0.94059014 0.8583485\n",
      " 1.055479   0.87902224 0.9325279  0.9818203  0.93738055 0.8669917\n",
      " 0.09217417 0.02813121 0.36425203 0.3625567  0.1001415  0.02094238\n",
      " 0.7       ] \n",
      " Balance 674.867857998684\n",
      "Act: 1 \n",
      " Obs: [0.8705201  0.83626133 0.9249161  0.98429286 0.91705555 1.0535014\n",
      " 0.87012416 0.9214736  0.84228754 0.8543357  0.9793527  0.96052766\n",
      " 0.92657924 1.0515907  0.8690637  0.9107214  0.92878926 0.84836733\n",
      " 1.0504756  0.8785763  0.91893035 0.96888506 0.9230542  0.85480636\n",
      " 0.11041603 0.03369856 0.3650608  0.30616152 0.0796768  0.02783562\n",
      " 0.16266666] \n",
      " Balance 1019.0728666511799\n",
      "Act: 0 \n",
      " Obs: [0.8580723  0.821404   0.9240574  0.9782992  0.91725963 1.0415366\n",
      " 0.8585764  0.91116726 0.83448017 0.8460623  0.973828   0.955272\n",
      " 0.9143661  1.0423931  0.84727734 0.901421   0.9166879  0.84393734\n",
      " 1.0414907  0.86212146 0.9045972  0.95522857 0.91462827 0.8369124\n",
      " 0.10958076 0.03344363 0.36527574 0.26610023 0.05645774 0.01838419\n",
      " 0.22666667] \n",
      " Balance 1340.4706682243545\n",
      "Act: 0 \n",
      " Obs: [0.85194504 0.8085073  0.91339475 0.9780811  0.9086429  1.0423404\n",
      " 0.84874445 0.8969582  0.82491386 0.83991647 0.9707404  0.94840074\n",
      " 0.9083341  1.0313619  0.8334102  0.8902053  0.91823214 0.8325708\n",
      " 1.0291224  0.84887606 0.8957669  0.94659764 0.904169   0.8255881\n",
      " 0.09775499 0.02983445 0.36599016 0.29206464 0.05758689 0.02723002\n",
      " 0.26266667] \n",
      " Balance 1368.4981739399814\n",
      "Act: 0 \n",
      " Obs: [0.8369087  0.8064923  0.905676   0.97684836 0.9033565  1.0391643\n",
      " 0.8345772  0.87743634 0.80863464 0.82676154 0.96474874 0.93799925\n",
      " 0.8978607  1.0284815  0.8246563  0.88815343 0.9089204  0.8214961\n",
      " 1.016926   0.84728867 0.8787639  0.93219835 0.89345396 0.8092035\n",
      " 0.10695103 0.03264105 0.4175376  0.26028982 0.05873863 0.04161472\n",
      " 0.788     ] \n",
      " Balance 1156.497020693723\n",
      "Act: 1 \n",
      " Obs: [0.83228135 1.0569443  0.90468836 0.9607693  0.89008564 1.0247533\n",
      " 0.8237104  0.8726394  0.7942196  0.8124323  0.9441415  0.92674214\n",
      " 0.8850918  1.0244678  0.82192206 0.87671316 0.89942825 0.81431276\n",
      " 1.0025371  0.83550394 0.861809   0.9141291  0.88610137 0.7925908\n",
      " 0.14380969 0.04389017 0.4185959  0.22227518 0.08537959 0.04411161\n",
      " 0.23466666] \n",
      " Balance 860.4941179436669\n",
      "Act: 0 \n",
      " Obs: [0.         1.0430591  0.8863609  0.95930547 0.8813332  1.0168965\n",
      " 0.80132294 0.85710776 0.78439504 0.8023461  0.92878795 0.9069714\n",
      " 0.87922615 1.0039169  0.80279887 0.8702914  0.8931416  0.80731976\n",
      " 0.9982078  0.82313675 0.8449398  0.90088415 0.86463135 0.7895981\n",
      " 0.15196571 0.04637936 0.41871446 0.18062262 0.06111167 0.02889317\n",
      " 0.588     ] \n",
      " Balance 1378.421186776673\n",
      "Act: 0 \n",
      " Obs: [0.         1.0352737  0.8697079  0.946706   0.8670003  1.0067886\n",
      " 0.         0.8438393  0.7824809  0.79707015 0.9191038  0.90110373\n",
      " 0.         0.9910169  0.7932128  0.86158955 0.8822645  0.8016413\n",
      " 0.9906273  0.81992406 0.8310988  0.88650537 0.8620894  0.77808726\n",
      " 0.16723523 0.05103956 0.41871637 0.14175673 0.05973665 0.01544022\n",
      " 0.172     ] \n",
      " Balance 1436.1240411386386\n",
      "Act: 0 \n",
      " Obs: [0.         1.0382004  0.8699895  0.9414652  0.86017627 1.0001636\n",
      " 0.         0.8390334  0.7673505  0.7864066  0.90242076 0.8900788\n",
      " 0.         0.9741787  0.7802913  0.8488462  0.         0.7927623\n",
      " 0.97565347 0.80771023 0.8185046  0.8706569  0.85343105 0.77329767\n",
      " 0.14632778 0.04465869 0.4508906  0.11895375 0.055633   0.03123741\n",
      " 0.43733335] \n",
      " Balance 1457.9826277750683\n",
      "Act: 0 \n",
      " Obs: [0.         1.0294383  0.8589622  0.92286944 0.853724   0.996192\n",
      " 0.         0.83806115 0.750275   0.77315396 0.88572764 0.89080447\n",
      " 0.         0.9562159  0.77042675 0.8472313  0.         0.\n",
      " 0.9728575  0.80117893 0.8065137  0.86635965 0.86339974 0.75842595\n",
      " 0.15906745 0.04854679 0.45089063 0.09137692 0.05404349 0.01721377\n",
      " 0.9346667 ] \n",
      " Balance 1066.064040775168\n",
      "Act: 1 \n",
      " Obs: [1.0584882  1.0188543  0.85665965 0.9007092  0.8416568  0.9841151\n",
      " 0.         0.83015287 0.73654336 0.76931685 0.8752489  0.8811405\n",
      " 0.         0.94631654 0.75092155 0.83504635 0.         0.\n",
      " 0.96355224 0.7860416  0.79265517 0.85491544 0.8496651  0.7383704\n",
      " 0.15906745 0.04854679 0.45089063 0.09137692 0.10265771 0.0182466\n",
      " 0.19733334] \n",
      " Balance 795.0141686017348\n",
      "total financial balance: (eur) 19244.282041594794 internal rate of return 10.429591685791117 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate1(1, env_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b1e177",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate2(episodes, environment, model):\n",
    "    \n",
    "    mean_irr = 0\n",
    "    mean_fin_balance = 0\n",
    "    irr = 0\n",
    "    fin_balance = 0\n",
    "    count = 0\n",
    "    npv = 0\n",
    "    list_npv = []\n",
    "    env_balance = 0\n",
    "    mean_env_balance = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)  # Now obs is just the observation array\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            keys = list(info.keys())\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            second_value = values[1]\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            third_value = values[2]\n",
    "            fourth_value = values[4]\n",
    "            fith_value = values[5]\n",
    "        \n",
    "        fin_balance += second_value\n",
    "        npv += fourth_value\n",
    "        count += 1\n",
    "        env_balance += fith_value\n",
    "        \n",
    "        #list_npv.append(fourth_value)\n",
    "            \n",
    "    mean_fin_balance = fin_balance/count\n",
    "    mean_npv = npv/count\n",
    "    mean_env_balance = env_balance / count\n",
    "\n",
    "\n",
    "    environment.close()\n",
    "    \n",
    "    return(mean_npv, mean_env_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "218f1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_3424\\1296066225.py:218: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10235.56755559278, 19099.413178032464)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate2(1000, env_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "760ee4e7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def basepolicy1(episodes, environment):\n",
    "    \n",
    "    mean_irr = 0\n",
    "    mean_fin_balance = 0\n",
    "    irr = 0\n",
    "    fin_balance = 0\n",
    "    count = 0\n",
    "    irr_count = 0\n",
    "    npv = 0\n",
    "    list_npv = []\n",
    "    env_balance = 0\n",
    "    mean_env_balance = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = 0\n",
    "            for i, n in enumerate(obs):\n",
    "                if i < 24:\n",
    "                    if n < 0.80:\n",
    "                        action += 1\n",
    "\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            keys = list(info.keys())\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            second_value = values[1]\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            third_value = values[2]\n",
    "            fourth_value = values[4]\n",
    "            fith_value = values[5]\n",
    "        \n",
    "        fin_balance += second_value\n",
    "        npv += fourth_value\n",
    "        count += 1\n",
    "        env_balance += fith_value\n",
    "        \n",
    "        list_npv.append(fourth_value)\n",
    "            \n",
    "    mean_fin_balance = fin_balance/count\n",
    "    mean_npv = npv/count\n",
    "    mean_env_balance = env_balance/count\n",
    "\n",
    "    environment.close()\n",
    "    \n",
    "    print(mean_npv, mean_env_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94ddefc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_3424\\1296066225.py:218: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14883.575086659153 32830.9773533861\n"
     ]
    }
   ],
   "source": [
    "basepolicy1(1000, env_test)\n",
    "\n",
    "# 19402.381263116342 35075.88501505905"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
