{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7702da",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from environment_fx_no_env import calculate_import_export, test1, test2, test3, evaluate1, evaluate2, basepolicy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f3f975",
   "metadata": {
    "code_folding": [
     22
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# import and modify data\n",
    "\n",
    "# Assuming the file is a CSV and specifying the correct path and filename\n",
    "file_path = r\"file_path\"\n",
    "\n",
    "# Use pandas to read the CSV file\n",
    "AC_OUTPUT = pd.read_csv(file_path + \"/AC_OUTPUT_JA\")\n",
    "elec_df = pd.read_csv(file_path + \"/hourly_consumption_gemany.csv\")\n",
    "import_price = pd.read_csv(file_path + \"/electricity_tariff.csv\")\n",
    "\n",
    "#elec_df = elec_df * 1000\n",
    "elec_df = elec_df.drop('HourOfYear', axis=1)\n",
    "\n",
    "elec_df['hour_of_day'] = np.arange(8760) % 24\n",
    "elec_df['day_of_week'] = np.arange(8760) // 24 % 7  # 0 is Monday, 6 is Sunday\n",
    "\n",
    "# Define rates\n",
    "peak_rate = 1.45\n",
    "normal_rate = 1\n",
    "off_peak_rate = 0.85\n",
    "\n",
    "# Function to determine rate based on hour and day\n",
    "def determine_rate(hour, day):\n",
    "    if day < 5:  # Monday to Friday\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return peak_rate\n",
    "        elif 6 <= hour < 10:  # 7am to 9am and 10am to 3pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    else:  # Weekend\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    \n",
    "# Apply the function to each row to determine the rate\n",
    "elec_df['rate'] = elec_df.apply(lambda row: determine_rate(row['hour_of_day'], row['day_of_week']), axis=1)\n",
    "\n",
    "import_price_df = import_price.drop(columns=['x'])\n",
    "import_price_df = import_price_df[:-26]\n",
    "\n",
    "train_cols = random.sample(list(import_price_df.columns), 7000)\n",
    "import_price_train = import_price_df[train_cols]\n",
    "test_cols = [col for col in import_price_df.columns if col not in train_cols]\n",
    "import_price_test = import_price_df[test_cols]\n",
    "\n",
    "Eff = pd.read_csv(file_path + \"/Efficency_impr\")\n",
    "Eff = (Eff)/100 + 1\n",
    "\n",
    "CAPEX = pd.read_csv(file_path + \"/CAPEX_JA.csv\")\n",
    "CAPEX_JA = (CAPEX[:26]) * 1.3\n",
    "\n",
    "\n",
    "train_cols_CAPEX = random.sample(list(CAPEX_JA.columns), 7000)\n",
    "test_cols_CAPEX = [col for col in CAPEX_JA.columns if col not in train_cols_CAPEX]\n",
    "\n",
    "CAPEX_JA_train = CAPEX_JA[train_cols_CAPEX]\n",
    "CAPEX_JA_test = CAPEX_JA[test_cols_CAPEX]\n",
    "\n",
    "train_cols_Eff = random.sample(list(Eff.columns), 7000)\n",
    "test_cols_Eff = [col for col in Eff.columns if col not in train_cols_Eff]\n",
    "\n",
    "Eff_train = Eff[train_cols_Eff]\n",
    "Eff_test = Eff[test_cols_Eff]\n",
    "\n",
    "AC_OUTPUT_arr = (np.array(AC_OUTPUT.T)).flatten()\n",
    "\n",
    "Eff_train_arr = np.array(Eff_train.T)\n",
    "Eff_test_arr = np.array(Eff_test.T)\n",
    "\n",
    "CAPEX_JA_train_arr = np.array(CAPEX_JA_train.T)\n",
    "CAPEX_JA_test_arr = np.array(CAPEX_JA_test.T)\n",
    "\n",
    "elec_consum_arr = np.array(elec_df[\"Consumption\"])\n",
    "import_price_rate = np.array(elec_df[\"rate\"])\n",
    "\n",
    "import_price_train_arr = np.array(import_price_train.T)\n",
    "import_price_test_arr = np.array(import_price_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e01482e1",
   "metadata": {
    "code_folding": [
     0,
     49
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class TrainEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 24\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 0.82 # Trina: 1.19, JA: 0.82, Maxeon: 0.67\n",
    "        self.deg_std = 0.555 \n",
    "        \n",
    "        self.phi = 30 # Trina: 15, JA: 30, Maxeon: 50\n",
    "\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415  # Trina: 265, JA: 415, Maxeon: 435\n",
    "        self.cost_per_Wp_df_at_zero = 0.69 # Trina: 0.36, JA: 0.69, Maxeon: 1.58\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        \n",
    "        self.loan_interest_rate = 1.10\n",
    "        self.normal_interest_rate = 1.02\n",
    "        \n",
    "        self.low_budget = 0 # Low budget: 0, High Budget: 750\n",
    "        self.high_budget = 750 # Low budget: 750, High Budget: 1500\n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 7,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min()) / (self.import_price_df.max().max() - self.import_price_df.min().min())\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min() * 0.33) / (self.import_price_df.max().max() - self.import_price_df.min().min() * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(self.low_budget, self.high_budget)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0.]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(self.low_budget, self.high_budget) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - self.low_budget * self.step_total_interest) / (self.high_budget * self.step_total_interest - self.low_budget * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * self.phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(8)\n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        \n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    " \n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "\n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        #reward = self.fin_balance_tot / 1000 if done else 0\n",
    "        reward = self.fin_balance / 1000\n",
    "        \n",
    "        # FAILURE\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        survival = self.failure(actions_step)\n",
    "        \n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": 0,\n",
    "               \"current_interest\": resale,\n",
    "                \"net present value\": 0}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff95454",
   "metadata": {
    "code_folding": [
     0,
     45,
     231,
     421
    ]
   },
   "outputs": [],
   "source": [
    "class TestEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 24\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 0.82 # Trina: 1.19, JA: 0.82, Maxeon: 0.67\n",
    "        self.deg_std = 0.555 \n",
    "        \n",
    "        self.phi = 30 # Trina: 15, JA: 30, Maxeon: 50\n",
    "\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415  # Trina: 265, JA: 415, Maxeon: 435\n",
    "        self.cost_per_Wp_df_at_zero = 0.69 # Trina: 0.36, JA: 0.69, Maxeon: 1.58\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        \n",
    "        self.loan_interest_rate = 1.10\n",
    "        self.normal_interest_rate = 1.02\n",
    "        \n",
    "        self.low_budget = 0 # Low budget: 0, High Budget: 750\n",
    "        self.high_budget = 750 # Low budget: 750, High Budget: 1500\n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 7,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min()) / (self.import_price_df.max().max() - self.import_price_df.min().min())\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - self.import_price_df.min().min() * 0.33) / (self.import_price_df.max().max() - self.import_price_df.min().min() * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(self.low_budget, self.high_budget)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0.]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "    \n",
    "    def calculate_irr_and_npv(self, pv_cost, minimised_revenue, export_revenue, penalty):\n",
    "                \n",
    "        \"\"\"\n",
    "        Calculates total cash flow of the project needed for the internal rate of return\n",
    "        \"\"\" \n",
    "        self.expences = 0\n",
    "        self.annual_cash_flow = 0\n",
    "        initial_cost = 0\n",
    "        \n",
    "        self.expences = pv_cost\n",
    "        self.annual_cash_flow = self.expences + export_revenue + minimised_revenue + penalty\n",
    "        initial_cost_q, x = self.calculate_total_CAPEX(self.init_obs, self.panel_cost_and_inverter)\n",
    "        initial_cost = - initial_cost_q\n",
    "        \n",
    "        if self.episode_len == 24:\n",
    "            self.total_cash_flow.append(initial_cost + self.annual_cash_flow) \n",
    "        else:\n",
    "            self.total_cash_flow.append(self.annual_cash_flow) \n",
    "        \n",
    "        return self.total_cash_flow\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(self.low_budget, self.high_budget) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - self.low_budget * self.step_total_interest) / (self.high_budget * self.step_total_interest - self.low_budget * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * self.phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        irr_fin = 0\n",
    "        npv_fin = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(self.number_of_panels + 1)\n",
    "        \n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    "\n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "        \n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        # CALCULATE IRR, NPV AND CARBON INTENSITY\n",
    "        total_cash_flow = self.calculate_irr_and_npv(self.pv_costs, exported_revenue, minimised_revenue, current_penalty)\n",
    "        irr = npf.irr(total_cash_flow) * 100\n",
    "        npv = npf.npv(0.04 ,total_cash_flow)\n",
    "            \n",
    "        # RETURNS AND CALCULATE REWARD\n",
    "        if self.episode_len == 0:\n",
    "            irr_fin = irr\n",
    "            npv_fin = npv\n",
    "        \n",
    "        reward = self.fin_balance / 1000\n",
    "        \n",
    "        # FAILURE\n",
    "         \n",
    "        survival = self.failure(actions_step)\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "\n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.cost_per_Wp_df_at_zero - self.cost_per_Wp_df.min().min()) / (self.cost_per_Wp_df.max().max() - self.cost_per_Wp_df.min().min())\n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "        \n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": irr_fin,\n",
    "               \"current_interest\": current_penalty,\n",
    "                \"net present value\": npv_fin}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c69028",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, Eff_train_arr, CAPEX_JA_train_arr)\n",
    "env_test = TestEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_test_arr, Eff_test_arr, CAPEX_JA_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f083fc0",
   "metadata": {
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class RewardNormalizer:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.M2 = 0\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.n += 1\n",
    "        delta = reward - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = reward - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        if self.n < 2:\n",
    "            return reward  # Not enough data to normalize\n",
    "        variance = self.M2 / (self.n - 1)\n",
    "        std_dev = variance ** 0.5\n",
    "        return (reward - self.mean) / std_dev \n",
    "\n",
    "    def sample_and_normalize_rewards(self, episodes, environment):\n",
    "        normalized_rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            obs = environment.reset()\n",
    "            step = 0\n",
    "            \n",
    "            while not done:\n",
    "                step += 1\n",
    "                random_action = environment.action_space.sample()\n",
    "                obs, reward, done, _, info = environment.step(random_action)\n",
    "                \n",
    "                values = list(info.values())\n",
    "\n",
    "                reward = values[0]\n",
    "\n",
    "                # Assuming the reward to normalize is directly obtained from the environment step\n",
    "                self.update(reward)\n",
    "                normalized_reward = self.normalize(reward)\n",
    "                normalized_rewards.append(normalized_reward)\n",
    "        \n",
    "        # Calculate the final standard deviation\n",
    "        final_std_dev = (self.M2 / (self.n - 1)) ** 0.5 if self.n > 1 else 0\n",
    "\n",
    "        return normalized_rewards, self.mean, final_std_dev\n",
    "\n",
    "normalizer = RewardNormalizer()\n",
    "normalized_rewards, final_mean, final_std_dev = normalizer.sample_and_normalize_rewards(1000, env)\"\"\"\n",
    "a=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60c5c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4903b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test3(1, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1279284b",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test1(1000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1e45550",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def make_env(rank: int, seed: int = 0) -> Callable:\n",
    "    def _init() -> gym.Env:\n",
    "        random.seed(seed + rank)\n",
    "        np.random.seed(seed + rank) \n",
    "        env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, Eff_train_arr, CAPEX_JA_train_arr)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "# Number of environments to run in parallel\n",
    "num_cpu = 16\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4053c411",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def logarithmic_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a logarithmically decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Avoid taking log of zero by setting a lower limit close to zero\n",
    "        epsilon = 0.0001\n",
    "        progress = max(epsilon, 1 - progress_remaining)\n",
    "        # Calculate the decay factor using a logarithmic scale\n",
    "        return final_value + (initial_value - final_value) * math.log(1/progress)\n",
    "    return func\n",
    "\n",
    "\n",
    "learning_rate = logarithmic_schedule(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36af5784",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "log_path = \"./logs/\"\n",
    "eval_callback = EvalCallback(env_test, best_model_save_path = \"C:/Users/kubaw/Desktop/DELFT/THESIS/CODE/TEST_MODELS/ja24_low_aa/\",\n",
    "                             log_path = log_path, n_eval_episodes = 750, eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41010d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[512, 512], vf=[512, 512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3408195a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a linearly decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Calculate the decrease based on the remaining progress\n",
    "        return final_value + (initial_value - final_value) * progress_remaining\n",
    "    return func\n",
    "\n",
    "# Define the learning rate using the linear schedule\n",
    "learning_rate = linear_schedule(0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "678979b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\\PPO_422\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 4939  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_3428\\2392353474.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=47232, episode_reward=28.07 +/- 10.99\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 28.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018956501 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.2        |\n",
      "|    explained_variance   | -0.0014     |\n",
      "|    learning_rate        | 0.000299    |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 81.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1015  |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 920         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019049384 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.15       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.000298    |\n",
      "|    loss                 | 34.5        |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=127232, episode_reward=33.36 +/- 12.89\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 33.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019104427 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.000296    |\n",
      "|    loss                 | 38.9        |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 751    |\n",
      "|    iterations      | 4      |\n",
      "|    time_elapsed    | 174    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858328 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.000295    |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 82.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019415446 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 71          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=207232, episode_reward=38.64 +/- 13.74\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 207232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021048881 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.000293    |\n",
      "|    loss                 | 33.8        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 65.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 718    |\n",
      "|    iterations      | 7      |\n",
      "|    time_elapsed    | 319    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 720         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019978723 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.000292    |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=287232, episode_reward=36.98 +/- 12.94\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | 37        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 287232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0176042 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.44     |\n",
      "|    explained_variance   | 0.363     |\n",
      "|    learning_rate        | 0.00029   |\n",
      "|    loss                 | 23.7      |\n",
      "|    n_updates            | 192       |\n",
      "|    policy_gradient_loss | -0.0259   |\n",
      "|    value_loss           | 55.4      |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 689    |\n",
      "|    iterations      | 9      |\n",
      "|    time_elapsed    | 427    |\n",
      "|    total_timesteps | 294912 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 687         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 476         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015668442 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.31       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.000289    |\n",
      "|    loss                 | 26.3        |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 53.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 684         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016118238 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 52.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=367232, episode_reward=35.69 +/- 12.91\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 367232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016713824 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.09       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 264         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 49.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 645    |\n",
      "|    iterations      | 12     |\n",
      "|    time_elapsed    | 609    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 657         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015374155 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.000286    |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 47.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=447232, episode_reward=38.32 +/- 13.30\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 447232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014111632 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.000285    |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 14     |\n",
      "|    time_elapsed    | 728    |\n",
      "|    total_timesteps | 458752 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 775         |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011200059 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.000283    |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | -0.00915    |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011829674 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00857    |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=527232, episode_reward=36.55 +/- 13.06\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 527232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011934823 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.000281    |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 384         |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 628    |\n",
      "|    iterations      | 17     |\n",
      "|    time_elapsed    | 885    |\n",
      "|    total_timesteps | 557056 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 928         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001693 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00028     |\n",
      "|    loss                 | 27.4        |\n",
      "|    n_updates            | 408         |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=607232, episode_reward=36.93 +/- 13.57\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 36.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 607232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010371079 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.000279    |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 432         |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    value_loss           | 47.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 626    |\n",
      "|    iterations      | 19     |\n",
      "|    time_elapsed    | 993    |\n",
      "|    total_timesteps | 622592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 1042        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009137249 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.000277    |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 456         |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=687232, episode_reward=36.91 +/- 13.28\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 36.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 687232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008595929 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.000276    |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    value_loss           | 42.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 620    |\n",
      "|    iterations      | 21     |\n",
      "|    time_elapsed    | 1109   |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1153        |\n",
      "|    total_timesteps      | 720896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008940687 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 504         |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1201        |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010273358 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.000274    |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 528         |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=767232, episode_reward=39.15 +/- 13.76\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 39.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 767232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009270728 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.000273    |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 552         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 46.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 617    |\n",
      "|    iterations      | 24     |\n",
      "|    time_elapsed    | 1274   |\n",
      "|    total_timesteps | 786432 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1323        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010260252 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 576         |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=847232, episode_reward=38.28 +/- 13.75\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 847232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009511864 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.00027     |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 47.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 609    |\n",
      "|    iterations      | 26     |\n",
      "|    time_elapsed    | 1398   |\n",
      "|    total_timesteps | 851968 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 612          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1443         |\n",
      "|    total_timesteps      | 884736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093309255 |\n",
      "|    clip_fraction        | 0.0966       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.628        |\n",
      "|    learning_rate        | 0.000269     |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 624          |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1488        |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007910922 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.000268    |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 648         |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=927232, episode_reward=38.48 +/- 14.20\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009129325 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.000267    |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 672         |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    value_loss           | 41.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 29     |\n",
      "|    time_elapsed    | 1561   |\n",
      "|    total_timesteps | 950272 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1608        |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009700989 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.000266    |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 696         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1007232, episode_reward=38.82 +/- 14.01\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 38.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1007232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088937525 |\n",
      "|    clip_fraction        | 0.0854       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.633        |\n",
      "|    learning_rate        | 0.000264     |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    value_loss           | 44.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 604     |\n",
      "|    iterations      | 31      |\n",
      "|    time_elapsed    | 1681    |\n",
      "|    total_timesteps | 1015808 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1727        |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008144934 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 744         |\n",
      "|    policy_gradient_loss | -0.00374    |\n",
      "|    value_loss           | 42.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1776        |\n",
      "|    total_timesteps      | 1081344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008278934 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.000262    |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 768         |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1087232, episode_reward=39.18 +/- 13.66\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 39.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1087232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008235468 |\n",
      "|    clip_fraction        | 0.0857      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.000261    |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 792         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    value_loss           | 42.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 604     |\n",
      "|    iterations      | 34      |\n",
      "|    time_elapsed    | 1844    |\n",
      "|    total_timesteps | 1114112 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 605         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1893        |\n",
      "|    total_timesteps      | 1146880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009694484 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 816         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1167232, episode_reward=38.38 +/- 13.60\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1167232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008016029 |\n",
      "|    clip_fraction        | 0.0796      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.000258    |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 600     |\n",
      "|    iterations      | 36      |\n",
      "|    time_elapsed    | 1963    |\n",
      "|    total_timesteps | 1179648 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 603         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 2009        |\n",
      "|    total_timesteps      | 1212416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008244311 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 864         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 605         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 2057        |\n",
      "|    total_timesteps      | 1245184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008091705 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.000256    |\n",
      "|    loss                 | 21.3        |\n",
      "|    n_updates            | 888         |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1247232, episode_reward=39.30 +/- 13.03\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 39.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1247232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074312994 |\n",
      "|    clip_fraction        | 0.0731       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.000255     |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 912          |\n",
      "|    policy_gradient_loss | -0.00341     |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 601     |\n",
      "|    iterations      | 39      |\n",
      "|    time_elapsed    | 2123    |\n",
      "|    total_timesteps | 1277952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 604         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 2167        |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008153155 |\n",
      "|    clip_fraction        | 0.0716      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000254    |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1327232, episode_reward=38.77 +/- 13.34\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1327232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007507886 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 601     |\n",
      "|    iterations      | 41      |\n",
      "|    time_elapsed    | 2234    |\n",
      "|    total_timesteps | 1343488 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 603         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 2279        |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007922545 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 984         |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1407232, episode_reward=38.14 +/- 13.64\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 38.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1407232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008305285 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 1008        |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 35.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 600     |\n",
      "|    iterations      | 43      |\n",
      "|    time_elapsed    | 2346    |\n",
      "|    total_timesteps | 1409024 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 602        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 2391       |\n",
      "|    total_timesteps      | 1441792    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00861172 |\n",
      "|    clip_fraction        | 0.0737     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.858     |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.000249   |\n",
      "|    loss                 | 15.7       |\n",
      "|    n_updates            | 1032       |\n",
      "|    policy_gradient_loss | -0.00371   |\n",
      "|    value_loss           | 34.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, learning_rate = learning_rate, batch_size = 256, n_epochs = 24, policy_kwargs = policy_kwargs, gamma = 0.99,  verbose=1, tensorboard_log = \"C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\")\n",
    "TIMESTEPS = 8000000\n",
    "model.learn(total_timesteps = TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5c9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69a9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_high.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69968a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act: 9 \n",
      " Obs: [0.8506809  0.8584513  0.9979332  0.9444468  0.94620717 0.8702701\n",
      " 0.89418256 0.9858364  0.99208087 0.         0.         0.9894404\n",
      " 0.9902745  0.9319078  0.9669751  0.930064   0.9864388  0.9897222\n",
      " 0.8567538  0.99934334 0.9690382  0.9883636  0.98566294 0.96554\n",
      " 0.07780816 0.08083121 0.04069865 0.6843568  0.22262634 0.06385198\n",
      " 0.4304    ] \n",
      " Balance 934.6484733204852\n",
      "Act: 1 \n",
      " Obs: [0.83448714 0.84543145 0.9839316  0.92941517 0.9384754  0.8553898\n",
      " 0.88254833 0.9720743  0.97176784 0.99386346 0.         0.98887855\n",
      " 0.9821276  0.918809   0.9589939  0.9213087  0.971079   0.98773867\n",
      " 0.8422795  0.9796777  0.9611708  0.97831863 0.9790401  0.9443\n",
      " 0.0921257  0.08930569 0.04105865 0.6808497  0.09113253 0.0676831\n",
      " 0.9064    ] \n",
      " Balance 1145.615082726474\n",
      "Act: 3 \n",
      " Obs: [0.98640937 0.8348458  0.97125524 0.91823715 0.92682475 0.8490568\n",
      " 0.8809269  0.96460253 0.964308   0.98867255 0.9894532  0.97657263\n",
      " 0.974911   0.9099361  0.9591434  0.9067026  0.9565509  0.9706006\n",
      " 0.9933011  0.9692458  0.94500977 0.9636699  0.9730638  0.9357928\n",
      " 0.09561976 0.0552629  0.04106111 0.5943884  0.14136715 0.07174408\n",
      " 0.5552    ] \n",
      " Balance 940.3085509209191\n",
      "Act: 2 \n",
      " Obs: [0.97475624 0.9986366  0.9594475  0.9080488  0.9212707  0.9913041\n",
      " 0.86800444 0.9559388  0.9587844  0.97639394 0.96464133 0.97236705\n",
      " 0.9660781  0.897618   0.9488707  0.8924981  0.9341199  0.95408237\n",
      " 0.9879687  0.9632052  0.9203248  0.9510968  0.96292406 0.9245241\n",
      " 0.10472413 0.07794939 0.04107094 0.5966404  0.09293696 0.07604873\n",
      " 0.1336    ] \n",
      " Balance 949.7748806354036\n",
      "Act: 1 \n",
      " Obs: [0.9634993  0.9809248  0.9484214  0.8948521  0.9177361  0.97815603\n",
      " 0.99794745 0.95031995 0.94084823 0.9526498  0.9592584  0.96955997\n",
      " 0.9610963  0.89392686 0.929588   0.88909996 0.9157731  0.9394618\n",
      " 0.98454905 0.95601743 0.9063338  0.93606067 0.95009947 0.91686004\n",
      " 0.10983434 0.04207152 0.04107323 0.57108706 0.06590144 0.\n",
      " 0.1368    ] \n",
      " Balance 1529.4394942791862\n",
      "Act: 1 \n",
      " Obs: [0.9483016  0.9670877  0.9330485  0.898819   0.9055777  0.9678885\n",
      " 0.97761977 0.93803096 0.93227535 0.9348593  0.94826454 0.95512635\n",
      " 0.9377264  0.8871137  0.9258887  0.9924407  0.89837676 0.92779344\n",
      " 0.9728134  0.9451508  0.8918613  0.92522395 0.94390833 0.91225404\n",
      " 0.11019813 0.03363205 0.07602565 0.56946665 0.06698449 0.\n",
      " 0.9288    ] \n",
      " Balance 1642.1572253391405\n",
      "Act: 4 \n",
      " Obs: [0.9368994  0.9627754  0.916609   1.0013274  0.89407086 0.95489484\n",
      " 0.9633136  0.92683613 0.91717213 0.9250872  0.9251594  0.93997025\n",
      " 0.9289183  1.0002288  0.9146217  0.9690141  1.0025158  0.9176952\n",
      " 0.95550424 0.93707097 1.0048509  0.9023107  0.9292881  0.9105177\n",
      " 0.13504967 0.04121665 0.13812822 0.51394975 0.13299784 0.\n",
      " 0.0152    ] \n",
      " Balance 890.3404174180982\n",
      "Act: 1 \n",
      " Obs: [0.9299922  0.9499109  0.90906024 0.99682856 1.0104346  0.9346675\n",
      " 0.9409741  0.9143583  0.9019747  0.9169     0.9120895  0.92545456\n",
      " 0.91869867 0.9864659  0.9018668  0.9654691  0.99789274 0.90338385\n",
      " 0.9453975  0.9248     0.98188025 0.89667404 0.91543025 0.8972186\n",
      " 0.14183883 0.04328867 0.13812873 0.5064812  0.069258   0.\n",
      " 0.0344    ] \n",
      " Balance 1743.214934666042\n",
      "Act: 1 \n",
      " Obs: [0.92655426 0.94064623 0.9034708  0.98114413 1.0012089  0.9322793\n",
      " 0.9284936  0.9077178  0.8910838  0.9072404  0.901143   0.91539264\n",
      " 0.90711737 0.98401344 0.8957346  0.94560874 0.98844326 0.8954661\n",
      " 0.93706214 0.9165231  0.97746134 1.0193481  0.9043004  0.89017534\n",
      " 0.1448572  0.04420987 0.16420986 0.49317336 0.07051323 0.\n",
      " 0.3664    ] \n",
      " Balance 1773.1536858645768\n",
      "Act: 1 \n",
      " Obs: [0.91080546 0.9297984  0.90872645 0.96863776 0.9975972  0.9123528\n",
      " 0.91501415 0.89558184 0.8758262  0.8926482  0.8901455  0.90447265\n",
      " 0.8928792  0.98391926 0.8851567  0.93233496 0.9738504  0.88316554\n",
      " 0.92648894 0.9198868  0.96316516 1.0102067  0.8912413  1.0106575\n",
      " 0.18209831 0.05557572 0.16420986 0.4847201  0.07177961 0.\n",
      " 0.14      ] \n",
      " Balance 1779.3282556174663\n",
      "Act: 2 \n",
      " Obs: [0.89807427 0.9164717  0.89748055 0.95728123 0.99585795 0.9082951\n",
      " 0.90013206 0.88811904 1.0036814  0.8830705  0.8880513  0.89312845\n",
      " 0.88829756 0.96852064 0.87255543 0.9228805  0.9694412  1.0108367\n",
      " 0.9153479  0.9086763  0.95677793 0.99997276 0.87949765 0.99807036\n",
      " 0.18525091 0.05653788 0.16974749 0.50560796 0.0941653  0.\n",
      " 0.4184    ] \n",
      " Balance 1910.0353453395373\n",
      "Act: 2 \n",
      " Obs: [0.88759315 0.9037864  0.8889607  0.95514446 0.9795166  0.90302086\n",
      " 0.9062131  0.87677157 0.9899941  0.87285024 0.8692387  0.87159914\n",
      " 0.8695483  0.9513401  0.99974644 0.9051399  0.95826167 1.0026973\n",
      " 0.9083333  0.89072895 0.94477403 0.9927929  1.0106494  0.99034625\n",
      " 0.18813035 0.05741667 0.18077813 0.5319831  0.09611496 0.\n",
      " 0.8824    ] \n",
      " Balance 1928.844353431066\n",
      "Act: 2 \n",
      " Obs: [0.87070125 0.88828903 0.88046306 0.9504382  0.9815592  0.88919514\n",
      " 0.90166485 0.86697906 0.98415935 0.85995626 1.0156606  0.8621396\n",
      " 1.0077194  0.9381767  0.9783532  0.9079018  0.9367571  1.0012172\n",
      " 0.8954606  0.8819654  0.93148977 0.99035794 0.9977789  0.9875441\n",
      " 0.19757754 0.06029992 0.18077831 0.45912147 0.09816764 0.\n",
      " 0.5504    ] \n",
      " Balance 1939.5120652570542\n",
      "Act: 1 \n",
      " Obs: [0.85413975 0.8786904  0.87193966 0.94128567 0.97761905 0.8996593\n",
      " 0.89375436 0.86664164 0.97681445 1.0075452  1.0014246  0.8529574\n",
      " 0.9910866  0.93084013 0.9683888  0.         0.9196819  0.9946947\n",
      " 0.88619834 0.8704636  0.92444307 0.9849563  0.98276585 0.9818179\n",
      " 0.22994629 0.07017874 0.18282528 0.46074983 0.07716084 0.\n",
      " 0.412     ] \n",
      " Balance 2226.928725784337\n",
      "Act: 1 \n",
      " Obs: [0.84943783 0.86565405 0.85494304 0.9277448  0.9725182  0.8932805\n",
      " 0.8803917  0.85214496 0.96076244 0.99489427 0.98490196 0.8448659\n",
      " 0.9725446  0.9207617  0.96197873 1.0102568  0.91260237 0.98596644\n",
      " 0.8853277  0.8528566  0.91085726 0.97572106 0.97963285 0.97616744\n",
      " 0.24595335 0.07506403 0.28252518 0.47136122 0.10230409 0.\n",
      " 0.18      ] \n",
      " Balance 2263.9147279586086\n",
      "Act: 2 \n",
      " Obs: [1.0363737  0.85582274 0.8512335  0.9147751  0.9714938  0.8818263\n",
      " 0.8662651  0.830817   0.94536656 0.9768403  0.97530574 1.0425062\n",
      " 0.96955717 0.9167655  0.95158786 0.9972227  0.90009344 0.9691526\n",
      " 0.8756043  0.84509134 0.9009377  0.97060424 0.967935   0.95568097\n",
      " 0.2605994  0.07953396 0.2825252  0.4092348  0.11096194 0.\n",
      " 0.1536    ] \n",
      " Balance 2352.4246379543447\n",
      "Act: 1 \n",
      " Obs: [1.023883   0.85001147 0.84165406 0.90085334 0.957925   0.8753978\n",
      " 0.8526361  1.0354875  0.9335544  0.9660458  0.9646987  1.0492685\n",
      " 0.9597043  0.90180993 0.94130355 0.9808908  0.8923459  0.9573265\n",
      " 0.85747576 0.8250363  0.8862272  0.95620024 0.95951843 0.94000435\n",
      " 0.28436795 0.08678803 0.2825252  0.38581574 0.08141283 0.\n",
      " 0.2944    ] \n",
      " Balance 2778.618799318506\n",
      "Act: 2 \n",
      " Obs: [1.0148448  0.8292031  1.0292662  0.8901795  0.948775   0.8620396\n",
      " 0.84160656 1.0288062  0.9162861  0.9575202  0.9568193  1.0403404\n",
      " 0.95126635 0.8931274  0.92973226 0.96195257 0.8803713  0.96188396\n",
      " 0.84876364 1.037806   0.880958   0.9389993  0.95191026 0.92391086\n",
      " 0.26504636 0.08089115 0.28879738 0.38479865 0.10593142 0.\n",
      " 0.7856    ] \n",
      " Balance 2788.124825333515\n",
      "Act: 1 \n",
      " Obs: [1.0064206  1.0329106  1.0197661  0.8777867  0.9410778  0.85783625\n",
      " 0.84016675 1.0134366  0.90313035 0.95359784 0.9335081  1.0270021\n",
      " 0.9357639  0.884642   0.9196825  0.9429521  0.86502486 0.9471668\n",
      " 0.842297   1.027471   0.8768915  0.920328   0.9389339  0.90814304\n",
      " 0.34865582 0.10640845 0.28879738 0.37200037 0.08440842 0.\n",
      " 0.3904    ] \n",
      " Balance 2812.5365274745113\n",
      "Act: 2 \n",
      " Obs: [0.9968674  1.0233434  1.0128655  0.86211956 0.93580055 0.8475305\n",
      " 1.03393    1.0002216  0.8946004  0.9460096  0.9250017  1.0135971\n",
      " 0.91945964 0.86524993 0.90635717 0.9328302  0.86082083 0.9342657\n",
      " 1.0326473  1.0215645  0.8676754  0.90661556 0.9169333  0.9026323\n",
      " 0.30265328 0.09236864 0.28879738 0.34739554 0.10975406 0.\n",
      " 0.824     ] \n",
      " Balance 3382.2610960459615\n",
      "Act: 5 \n",
      " Obs: [0.9874567  1.0126082  0.99612457 1.0339576  0.9315389  1.025369\n",
      " 1.0203562  0.9800379  0.8804229  0.9279206  0.92259157 0.99356693\n",
      " 0.90825105 1.0305207  0.901045   0.9240369  1.0337586  0.\n",
      " 1.0254498  1.0156585  1.0347673  0.9043691  0.9090624  0.8856208\n",
      " 0.35876018 0.10949225 0.29254502 0.34646842 0.17886198 0.\n",
      " 0.7552    ] \n",
      " Balance 2397.654362692555\n",
      "Act: 0 \n",
      " Obs: [0.9697684  1.0056604  0.9829626  1.0270052  0.92658293 1.0146888\n",
      " 1.0134263  0.96408033 0.86364555 0.91502774 0.9221052  0.9817934\n",
      " 0.9014586  1.0303715  0.88623565 0.91806996 1.0220178  0.\n",
      " 1.0147697  1.002886   1.0237784  0.8926114  0.89950645 0.8762761\n",
      " 0.33154726 0.10118698 0.29254583 0.3441164  0.05973665 0.\n",
      " 0.644     ] \n",
      " Balance 3845.268855176275\n",
      "Act: 0 \n",
      " Obs: [0.9529786  0.99016213 0.97092354 1.0150317  0.91784394 1.0020524\n",
      " 1.0084988  0.9573844  0.8592565  0.90433717 0.909034   0.97447914\n",
      " 0.892932   1.0183362  0.8870525  0.9072903  1.0143495  0.\n",
      " 1.0037078  0.98186934 1.0090767  0.         0.88931024 0.87020063\n",
      " 0.39055234 0.1191951  0.29254583 0.36268532 0.06093139 0.\n",
      " 0.348     ] \n",
      " Balance 3528.1740564274533\n",
      "Act: 0 \n",
      " Obs: [0.947183   0.98200756 0.95600486 1.0011257  0.9126527  0.982512\n",
      " 1.0002253  0.93950355 0.8573953  0.90791875 0.8908264  0.96260625\n",
      " 0.88289005 1.0096498  0.88302445 0.9005956  0.9982752  0.\n",
      " 1.0001295  0.9805163  0.99086046 0.         0.8772996  0.85589415\n",
      " 0.3983332  0.12156979 0.293019   0.3530706  0.05944784 0.\n",
      " 0.6712    ] \n",
      " Balance 3795.363189103803\n",
      "Act: 8 \n",
      " Obs: [0.94017863 0.9622266  0.94642293 0.9814183  0.8964538  0.9646598\n",
      " 0.99477    0.9243765  1.0410933  0.8952083  1.0318507  0.9552847\n",
      " 1.0337952  0.9965747  1.0381533  0.8854847  0.9867753  1.0324008\n",
      " 0.99734104 0.97816956 0.98739094 1.039977   1.030799   1.03187\n",
      " 0.3983332  0.12156979 0.293019   0.3530706  0.27670488 0.01320262\n",
      " 0.9296    ] \n",
      " Balance 2461.0126866457003\n",
      "total financial balance: (eur) 53738.65525473102 internal rate of return 28.65155125224854 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate1(1, env_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b1e177",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate2(episodes, environment, model):\n",
    "    \n",
    "    mean_irr = 0\n",
    "    mean_fin_balance = 0\n",
    "    irr = 0\n",
    "    fin_balance = 0\n",
    "    count = 0\n",
    "    npv = 0\n",
    "    list_npv = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)  # Now obs is just the observation array\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            keys = list(info.keys())\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            second_value = values[1]\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            third_value = values[2]\n",
    "            fourth_value = values[4]\n",
    "        \n",
    "        fin_balance += second_value\n",
    "        npv += fourth_value\n",
    "        count += 1\n",
    "        \n",
    "        list_npv.append(fourth_value)\n",
    "            \n",
    "    mean_fin_balance = fin_balance/count\n",
    "    mean_npv = npv/count\n",
    "\n",
    "    #print(mean_npv)\n",
    "\n",
    "    environment.close()\n",
    "    \n",
    "    return(list_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60446cca",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_24352\\2392353474.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    }
   ],
   "source": [
    "values_eval = evaluate2(5000, env_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
